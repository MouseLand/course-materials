{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRLjc08qpfJ0"
      },
      "source": [
        "# ðŸ§  Building simplified models of primary visual cortex (V1) ðŸ­ ðŸµ\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
        "https://colab.research.google.com/drive/1wZjPChsFO4dyLHIfH95d_IMeMwgsa3xK?usp=sharing)\n",
        "\n",
        "**By Fengtong(Farah) Du and Carsen Stringer**\n",
        "\n",
        "In this tutorial, we will build predictive models of V1 neural activity from visual stimuli (natural images). The predictive models are:\n",
        "\n",
        "1.   linear regression\n",
        "2.   one convolutional layer (shared across neurons) + neuron-specific readout\n",
        "3.   2-convolutional layer net + neuron-specific readout with separable convolutions\n",
        "4.   small neuron-specific \"minimodels\" with 2 convolutional layers + readout.\n",
        "\n",
        "For each of these models, we will visualize the weights to help us understand the feature selectivity of V1 neurons.\n",
        "\n",
        "This notebook uses [code](https://github.com/mouseland/minimodel) and [data](https://janelia.figshare.com/articles/dataset/Towards_a_simplified_model_of_primary_visual_cortex/28797638) from [Du et al 2025](https://www.nature.com/articles/s41467-025-61171-9). If you use any code or data from this notebook, please cite the paper,\n",
        "\n",
        "> **Total duration: 6â€“8 hours**\n",
        "\n",
        "To keep the notebook interactive, there are three types of exercises throughout\n",
        "\n",
        "1) QUESTION MARKS: where ????? need to be replaced by a short equation, such as a variable or a function name.\n",
        "\n",
        "2) DISCUSSION QUESTIONS (Q): have a short discussion with your colleague about this. At the end of each section, we will open the discussions to the whole group.\n",
        "\n",
        "3) QUIZ: multiple-choice that we take across the entire group. Keep track of your own points.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUi3IzlFpfJ1"
      },
      "source": [
        "# 0. Setup â±ï¸ ~5â€¯min  \n",
        "\n",
        "We will install the minimodel code, download the data, and define some helper functions for plotting and filters.\n",
        "\n",
        "In google colab, you will want to choose a runtime with a GPU (e.g. T4 option)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q6kZPKupfJ1"
      },
      "source": [
        "## 0.1 Install dependencies\n",
        "\n",
        "If you are running locally, you will want to remove the \"--no-deps\" flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EVn60KNpfJ1",
        "outputId": "89a948a8-041a-4673-fe87-4ee2afab36f9"
      },
      "outputs": [],
      "source": [
        "# ðŸ› ï¸ Install minimodel\n",
        "!pip install --no-deps git+https://github.com/mouseland/minimodel.git\n",
        "!pip install rastermap\n",
        "!pip install opt-einsum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbV4kdzApfJ2"
      },
      "source": [
        "## 0.2 Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stO8YXN5pfJ2",
        "outputId": "add943e9-a001-4138-ca5e-c634b7c57b8a"
      },
      "outputs": [],
      "source": [
        "# Create a data directory\n",
        "!mkdir -p data\n",
        "!mkdir -p checkpoints/fullmodel/\n",
        "\n",
        "# Download files\n",
        "# neural activity data\n",
        "!wget --header=\"User-Agent: Mozilla/5.0\" --header=\"Accept: text/html\" --header=\"Connection: keep-alive\" --content-disposition -P data \"https://figshare.com/ndownloader/files/53712311\"\n",
        "# natural images that were shown\n",
        "!wget --header=\"User-Agent: Mozilla/5.0\" --header=\"Accept: text/html\" --header=\"Connection: keep-alive\" --content-disposition -P data \"https://janelia.figshare.com/ndownloader/files/53678783\"\n",
        "# checkpoint for full model\n",
        "!wget -O checkpoints/fullmodel/FX10_051623_2layer_16_320_clamp_norm_depthsep_pool.pt https://github.com/MouseLand/minimodel/releases/download/V1.0.0/FX10_051623_2layer_16_320_clamp_norm_depthsep_pool.pt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSu0Sl02MjBH"
      },
      "source": [
        "## 0.3 Import libraries, declare GPU device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzasuW6EMmIq",
        "outputId": "e7c2e98b-048d-40a1-db4e-2d6fed9c7dab"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from matplotlib import patches\n",
        "\n",
        "# DECLARE DEVICE\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'Using torch device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS1j_dLZpfJ2"
      },
      "source": [
        "## 0.4 Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Cz2_bsuCpfJ2"
      },
      "outputs": [],
      "source": [
        "# @title run this cell to define the helper functions\n",
        "# remove spines in rcparams\n",
        "mpl.rcParams['axes.spines.right'] = False\n",
        "mpl.rcParams['axes.spines.top'] = False\n",
        "\n",
        "def show_stimulus(img, ax=None, show=False, filters=False, vmax=1):\n",
        "  \"\"\" Visualize a stimulus \"\"\"\n",
        "  if ax is None:\n",
        "    ax = plt.gca()\n",
        "  if filters:\n",
        "    ax.imshow(img, vmin=-vmax, vmax=vmax, cmap='RdBu_r')\n",
        "  else:\n",
        "    ax.imshow(img+0.5, cmap=\"gray\")\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  ax.spines['left'].set_visible(False)\n",
        "  ax.spines['bottom'].set_visible(False)\n",
        "  if show:\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_repeats(spks_test, ineuron):\n",
        "    \"\"\" plot repeats of stimulus presentations for ineuron\n",
        "    Args:\n",
        "        spks_test: neural activity, shape (nstim x nreps x n_neurons)\n",
        "        ineuron: index of neuron to plot\n",
        "    \"\"\"\n",
        "    nreps = spks_test.shape[1]\n",
        "    fig, ax = plt.subplots(nreps + 1, 1, figsize=(10, 5))\n",
        "    for i in range(nreps):\n",
        "        ax[i].plot(spks_test[:, i, ineuron], label=f'repeat {i+1}')\n",
        "        if i == 0:\n",
        "            ax[i].set_title(f'Neuron {ineuron}')\n",
        "        ax[i].text(-50, 1, f'repeat {i+1}')\n",
        "        ax[i].axis('off')\n",
        "\n",
        "    i += 1\n",
        "    sp_avg = spks_test[:, :, ineuron].mean(axis=1)\n",
        "    sp_std = spks_test[:, :, ineuron].std(axis=1)\n",
        "    ax[i].plot(sp_avg, color='k', lw=1)\n",
        "    ax[i].fill_between(np.arange(len(sp_avg)), sp_avg - sp_std, sp_avg + sp_std, color='k', alpha=0.2)\n",
        "    ax[i].text(-50, 1, 'average\\n +/- std')\n",
        "    ax[i].spines['left'].set_visible(False)\n",
        "    ax[i].spines['bottom'].set_visible(False)\n",
        "    ax[i].set_yticks([])\n",
        "    ax[i].set_xlabel('stimulus id')\n",
        "\n",
        "\n",
        "\n",
        "def plot_example_activations(stimuli, act, filters, channels=[0], vmax=10):\n",
        "  \"\"\" plot activations act and corresponding stimulus\n",
        "    Args:\n",
        "        stimuli: stimulus input to convolutional layer (n x h x w) or (h x w)\n",
        "        act: activations of convolutional layer (n_bins x conv_channels x n_bins)\n",
        "        filters: filters of convolutional layer (n_filters x 1 x K x K)\n",
        "        channels: which conv channels to plot\n",
        "\n",
        "  \"\"\"\n",
        "  if stimuli.ndim>2:\n",
        "    n_stimuli = stimuli.shape[0]\n",
        "  else:\n",
        "    stimuli = stimuli.unsqueeze(0)\n",
        "    n_stimuli = 1\n",
        "\n",
        "  fig, axs = plt.subplots(n_stimuli+1, 1 + len(channels), figsize=(18, 12))\n",
        "\n",
        "  ax = axs[0, 0]\n",
        "  ax.axis('off')\n",
        "\n",
        "  # plot filters\n",
        "  for i in range(len(filters)):\n",
        "    ax = axs[0, i+1]\n",
        "    show_stimulus(filters[i].squeeze(), ax=ax, filters=True)\n",
        "    ax.set_title(f'filter {i}')\n",
        "    ax.axis('off')\n",
        "\n",
        "  # plot stimulus\n",
        "  for i in range(n_stimuli):\n",
        "    show_stimulus(stimuli[i].squeeze(), ax=axs[i+1, 0])\n",
        "    axs[i+1, 0].set_title('stimulus')\n",
        "\n",
        "    # plot example activations\n",
        "    for k, (channel, ax) in enumerate(zip(channels, axs[i+1][1:])):\n",
        "      im = ax.imshow(act[i, channel], vmin=-vmax, vmax=vmax, cmap='RdBu_r')\n",
        "      ax.set_xlabel('x-pos')\n",
        "      ax.set_ylabel('y-pos')\n",
        "      ax.set_title(f'channel {channel}')\n",
        "\n",
        "    cb_ax = fig.add_axes([1.05, 0.8, 0.01, 0.1])\n",
        "    plt.colorbar(im, cax=cb_ax)\n",
        "    cb_ax.set_title('activation\\n strength')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def plot_correlation_maps(spks, act, filters, vmax=0.35):\n",
        "  \"\"\" plot correlation maps of neural activity with convolutional layer activations\n",
        "  Args:\n",
        "    spks: neural activity, shape (n_stimuli x n_reps x n_neurons)\n",
        "    act: activations of convolutional layer (n_neurons x conv_channels x h, w)\n",
        "    channels: which conv channels to plot\n",
        "  \"\"\"\n",
        "  n_stimuli, n_reps, n_neurons = spks.shape\n",
        "  n_neurons, n_filters, h, w = act.shape\n",
        "\n",
        "  fig, axs = plt.subplots(n_neurons+1, n_filters, figsize=(12, 6))\n",
        "\n",
        "  ax = axs[0, 0]\n",
        "  ax.axis('off')\n",
        "\n",
        "  # plot filters\n",
        "  for i in range(len(filters)):\n",
        "    ax = axs[0, i]\n",
        "    show_stimulus(filters[i].squeeze(), ax=ax, filters=True)\n",
        "    ax.set_title(f'filter {i}')\n",
        "    ax.axis('off')\n",
        "\n",
        "  # plot stimulus\n",
        "  for i in range(n_neurons):\n",
        "    ax.axis('off')\n",
        "\n",
        "    # plot example activations\n",
        "    channels = np.arange(n_filters)\n",
        "    for k, (channel, ax) in enumerate(zip(channels, axs[i+1])):\n",
        "      # add gaussian blur to the activation map\n",
        "      from scipy.ndimage import gaussian_filter\n",
        "      im = ax.imshow(act[i,k], vmin=-vmax, vmax=vmax, cmap='RdBu_r')\n",
        "      ax.axis('off')\n",
        "      if k == 0:\n",
        "        ax.set_title(f'neuron {i}', loc='left')\n",
        "\n",
        "\n",
        "    cb_ax = fig.add_axes([1.05, 0.8, 0.01, 0.1])\n",
        "    plt.colorbar(im, cax=cb_ax)\n",
        "    cb_ax.set_title('correlation')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def filters(out_channels=6, K=7):\n",
        "  \"\"\" make example filters, some center-surround and gabors\n",
        "  Returns:\n",
        "      filters: out_channels x K x K\n",
        "  \"\"\"\n",
        "  grid = np.linspace(-K/2, K/2, K).astype(np.float32)\n",
        "  xx,yy = np.meshgrid(grid, grid, indexing='ij')\n",
        "\n",
        "  # create center-surround filters\n",
        "  sigma = 1.1*2\n",
        "  gaussian = np.exp(-(xx**2 + yy**2)**0.5/(2*sigma**2))\n",
        "  wide_gaussian = np.exp(-(xx**2 + yy**2)**0.5/(2*(sigma*2)**2))\n",
        "  center_surround = gaussian - 0.5 * wide_gaussian\n",
        "\n",
        "  # create gabor filters\n",
        "  thetas = np.linspace(0, 360, out_channels-2+1)[:-1] * np.pi/180\n",
        "  gabors = np.zeros((len(thetas), K, K), np.float32)\n",
        "  lam = 10*2\n",
        "  phi = np.pi/2\n",
        "  gaussian = np.exp(-(xx**2 + yy**2)**0.5/(2*(sigma*0.4)**2))\n",
        "  for i,theta in enumerate(thetas):\n",
        "    x = xx*np.cos(theta) + yy*np.sin(theta)\n",
        "    gabors[i] = gaussian * np.cos(2*np.pi*x/lam + phi)\n",
        "\n",
        "  filters = np.concatenate((center_surround[np.newaxis,:,:],\n",
        "                            -1*center_surround[np.newaxis,:,:],\n",
        "                            gabors),\n",
        "                           axis=0)\n",
        "  filters /= np.abs(filters).max(axis=(1,2))[:,np.newaxis,np.newaxis]\n",
        "  filters -= filters.mean(axis=(1,2))[:,np.newaxis,np.newaxis]\n",
        "  # convert to torch\n",
        "  filters = torch.from_numpy(filters)\n",
        "  # add channel axis\n",
        "  filters = filters.unsqueeze(1)\n",
        "\n",
        "  return filters\n",
        "\n",
        "\n",
        "def plot_Wc(Wc_sort, conv_filters=None, vmax_filt=0.5, vmax_Wc=0.5):\n",
        "    fig, axs = plt.subplots(1 if conv_filters is None else 2, 1, figsize=(14,5))\n",
        "    ax = axs if conv_filters is None else axs[0]\n",
        "    ax.imshow(Wc_sort, cmap='RdBu_r', aspect=\"auto\", vmin=-vmax_Wc, vmax=vmax_Wc)\n",
        "    if conv_filters is not None:\n",
        "        ax.set_xticks(np.arange(Wc_sort.shape[1]))\n",
        "        ax.set_xticklabels(np.arange(1, Wc_sort.shape[1] + 1))\n",
        "    ax.set_xlabel('convolutional channels')\n",
        "    ax.set_ylabel('neurons')\n",
        "\n",
        "    if conv_filters is not None:\n",
        "        ax = axs[1]\n",
        "        vmax = 5e-1\n",
        "        K = conv_filters.shape[-1]\n",
        "        filt_cat = conv_filters.squeeze().transpose(1, 0, 2).reshape(K, -1)\n",
        "        ax.imshow(filt_cat, cmap='RdBu_r', vmin=-vmax_filt, vmax=vmax_filt)\n",
        "        for i in range(len(conv_filters)):\n",
        "            ax.axvline(i * K - 0.5, color='k', lw=1, ls='--')\n",
        "        ax.set_xticks(np.arange(len(conv_filters)) * K + K//2)\n",
        "        ax.set_xticklabels(np.arange(1, len(conv_filters) + 1));\n",
        "\n",
        "def plot_predictions(Y_test, Y_pred_linear, Y_pred_conv1, Y_pred_fullmodel, k=1):\n",
        "    Yt = Y_test[:,:,k].mean(axis=1)\n",
        "    titles = ['true', 'linear', 'conv+readout', '16-320+readout']\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(14, 4))\n",
        "    ax.plot(Yt, label='true', color='k', lw=2.5)\n",
        "    ax.plot(Y_pred_linear[:,k], lw=1)\n",
        "    ax.plot(Y_pred_conv1[:,k], lw=1)\n",
        "    ax.plot(Y_pred_fullmodel[:,k],  lw=1)\n",
        "    ax.set_ylabel('response')\n",
        "    ax.set_xlabel('stimulus')\n",
        "    plt.legend(titles, frameon=False, loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(14, 4))\n",
        "    colors = ['tab:blue', 'tab:orange', 'tab:green']\n",
        "    for i in range(3):\n",
        "        Yp = Y_pred_linear[:,k] if i==0 else Y_pred_conv1[:,k] if i==1 else Y_pred_fullmodel[:,k]\n",
        "        axs[i].scatter(Yt, Yp, c=colors[i])\n",
        "        axs[i].axis('square')\n",
        "        axs[i].set_ylabel(titles[i+1], color=colors[i])\n",
        "        axs[i].set_xlabel(titles[0])\n",
        "        axs[i].plot([0, Yt.max()], [0, Yt.max()], 'k--')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def add_channel_frame(axs, row, col_start, col_end, color, alpha):\n",
        "    ax = axs[row, col_start]  # Leftmost axis in the row\n",
        "    # Rectangle coordinates (x, y) and dimensions (width, height)\n",
        "    rect = patches.Rectangle(\n",
        "        (-0.025, -0.05), (col_end - col_start + 1)*1.2, 1.1, transform=ax.transAxes,\n",
        "        color=color, fill=False, linewidth=3, zorder=10, alpha=alpha,\n",
        "        clip_on=False  # To ensure it draws outside the axes\n",
        "    )\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "def plot_conv2_1x1(conv1_w, conv2_1x1, ichannels=[127,242, 102,68,204,217,293], Wc=None):\n",
        "    import matplotlib.colors as mcolors\n",
        "    norm = mcolors.Normalize(vmin=-1, vmax=1)\n",
        "    # normalize each channel\n",
        "    conv2_1x1_norm = conv2_1x1 / np.linalg.norm(conv2_1x1, axis=1, keepdims=True)\n",
        "    cmap = plt.cm.RdBu_r\n",
        "    nchannels = len(ichannels)\n",
        "    n_top = 8\n",
        "    fig, ax = plt.subplots(nchannels, n_top, figsize=(6, int(0.8*nchannels)))\n",
        "\n",
        "    for i in range(nchannels):\n",
        "        channel_w = conv2_1x1_norm[ichannels[i]]\n",
        "        idxes = np.argsort(np.abs(channel_w))[::-1][:n_top] # keep the top 8 channels\n",
        "        isort = np.argsort(channel_w[idxes])[::-1]\n",
        "        isort = idxes[isort]\n",
        "        for j in range(n_top):\n",
        "            ax[i, j].imshow(conv1_w[isort[j]], cmap='RdBu_r', vmin=-0.2, vmax=0.2)\n",
        "            ax[i, j].axis('off')\n",
        "\n",
        "            # Create the frame color\n",
        "            frame_color = cmap(norm(channel_w[isort[j]]))\n",
        "            alpha = abs(channel_w[isort[j]])\n",
        "            # Add a rectangle frame around the image\n",
        "            rect = plt.Rectangle(\n",
        "                (-1, -1),  # Starting point\n",
        "                conv1_w[isort[j]].shape[1]+1,  # Width\n",
        "                conv1_w[isort[j]].shape[0]+1,  # Height\n",
        "                linewidth=7, edgecolor=frame_color, facecolor='none'\n",
        "            )\n",
        "            ax[i, j].add_patch(rect)\n",
        "        if Wc is None:\n",
        "            ax[i, 0].text(-0.15, 0.4, f'channel {ichannels[i]+1}', transform=ax[i, 0].transAxes, ha=\"right\")\n",
        "        else:\n",
        "            ax[i, 0].text(-0.15, 0.4, '\\n'.join([f'channel {ichannels[i]+1}',f'w_c = {Wc[i]:.3f}']), transform=ax[i, 0].transAxes, ha=\"right\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_conv2(conv1_w, conv2_1x1, ichannels=[127,242, 102,68,204,217,293], Wc=None):\n",
        "    isort_channels = np.argsort(Wc[ichannels])[::-1]  # sort channels by Wc\n",
        "    ichannels = ichannels[isort_channels]  # sort channels by Wc\n",
        "    import matplotlib.colors as mcolors\n",
        "    norm = mcolors.Normalize(vmin=-1, vmax=1)\n",
        "    # normalize each channel\n",
        "    conv2_1x1_norm = conv2_1x1 / np.linalg.norm(conv2_1x1, axis=1, keepdims=True)\n",
        "    cmap = plt.cm.RdBu_r\n",
        "    nchannels = len(ichannels)\n",
        "\n",
        "    # visualize conv2 spatial\n",
        "    conv2_spatial = model.core.features.layer1.ds_conv.spatial_conv.weight.data.cpu().numpy().squeeze()\n",
        "\n",
        "\n",
        "    n_top = 8\n",
        "    fig, ax = plt.subplots(nchannels, n_top+1, figsize=(8, int(0.8*nchannels)))\n",
        "\n",
        "    for i in range(nchannels):\n",
        "        channel_w = conv2_1x1_norm[ichannels[i]]\n",
        "        idxes = np.argsort(np.abs(channel_w))[::-1][:n_top] # keep the top 8 channels\n",
        "        isort = np.argsort(channel_w[idxes])[::-1]\n",
        "        isort = idxes[isort]\n",
        "        for j in range(n_top):\n",
        "            ax[i, j].imshow(conv1_w[isort[j]], cmap='RdBu_r', vmin=-0.2, vmax=0.2)\n",
        "            # ax[i, j].set_title(f'{isort[j]:.0f}', Wfontsize=8)\n",
        "            ax[i, j].axis('off')\n",
        "\n",
        "            # Create the frame color\n",
        "            # print(norm(channel_w[isort[j]]))\n",
        "            frame_color = cmap(norm(channel_w[isort[j]]))\n",
        "            alpha = abs(channel_w[isort[j]])\n",
        "            # Add a rectangle frame around the image\n",
        "            rect = plt.Rectangle(\n",
        "                (-1, -1),  # Starting point\n",
        "                conv1_w[isort[j]].shape[1]+1,  # Width\n",
        "                conv1_w[isort[j]].shape[0]+1,  # Height\n",
        "                linewidth=7, edgecolor=frame_color, facecolor='none'\n",
        "            )\n",
        "            ax[i, j].add_patch(rect)\n",
        "        if Wc is None:\n",
        "            ax[i, 0].text(-0.15, 0.4, f'channel {ichannels[i]+1}', transform=ax[i, 0].transAxes, ha=\"right\")\n",
        "        else:\n",
        "            ax[i, 0].text(-0.15, 0.4, ' '.join([f'channel {ichannels[i]+1}',f'w_c = {Wc[ichannels[i]]:.3f}']), transform=ax[i, 0].transAxes, ha=\"right\")\n",
        "        ax[i, -1].imshow(conv2_spatial[ichannels[i]], cmap='RdBu_r', vmin=-0.2, vmax=0.2)\n",
        "        ax[i, -1].axis('off')\n",
        "    ax[0, -1].set_title('conv2\\nspatial', fontsize=10)\n",
        "    plt.show()\n",
        "\n",
        "def plot_maxstim(valid_wc, train_img, prediction_isort, channel_activity_isort, neuron_mask):\n",
        "    cmap = plt.get_cmap('plasma', 9)\n",
        "    # Parameters for the second plot\n",
        "    pad = 5\n",
        "    vmin = 0\n",
        "    vmax = 255\n",
        "\n",
        "    isort = np.argsort(valid_wc)[::-1]\n",
        "    Nchannel = np.min([len(valid_wc), 8])\n",
        "    Nimg = channel_activity_isort.shape[0]\n",
        "\n",
        "\n",
        "    # Combined plot layout\n",
        "    fig = plt.figure(figsize=(Nimg + 14, 8 * 1.1))\n",
        "    gs = plt.GridSpec(8, Nimg + 2, figure=fig, hspace=0.2, wspace=0.1)\n",
        "\n",
        "    # Plot one (4x4 grid on the left side, occupying 2 rows per row)\n",
        "    nshow = 16\n",
        "    for i in range(nshow):\n",
        "        row = (i // 2)\n",
        "        col = (i % 2)\n",
        "        ax = fig.add_subplot(gs[row, col])\n",
        "        ax.imshow(train_img[prediction_isort[i]].cpu().numpy().squeeze() * neuron_mask, cmap='gray', vmin=-1, vmax=1)\n",
        "        ax.axis('off')\n",
        "        if i == 0:\n",
        "            # add a rect around the 16 images\n",
        "            rect = patches.Rectangle(\n",
        "                (-0.03, -7.38-1.07), 2.275, 9.5, transform=ax.transAxes,\n",
        "                color='black', fill=False, linewidth=3, zorder=10,\n",
        "                clip_on=False  # To ensure it draws outside the axes\n",
        "            )\n",
        "            ax.add_patch(rect)\n",
        "            ax.text(-0.2, 1.3, 'neuron maximum stimuli', transform=ax.transAxes,\n",
        "                    verticalalignment='center', fontsize=24, color='black')\n",
        "\n",
        "    # Plot two (8xNimg grid on the right side)\n",
        "    axs = np.empty((8, Nimg), dtype=object)\n",
        "    for i in range(Nchannel):\n",
        "        if i < 6:\n",
        "            ichannel = i\n",
        "        else:\n",
        "            ichannel = -(Nchannel - i)\n",
        "        for j in range(Nimg):\n",
        "            axs[i, j] = fig.add_subplot(gs[i, j + 2])\n",
        "            axs[i, j].imshow(train_img[channel_activity_isort[j, isort[ichannel]]].cpu().numpy().squeeze() * neuron_mask, cmap='gray', vmin=-1, vmax=1)\n",
        "            axs[i, j].axis('off')\n",
        "        wc_value = valid_wc[isort[ichannel]]\n",
        "        # Determine the frame color and linewidth based on valid_wc[isort[ichannel]]\n",
        "        if wc_value > 0:\n",
        "            color = 'red'\n",
        "        else:\n",
        "            color = 'blue'\n",
        "        add_channel_frame(axs, i, 0, Nimg - 1, color, np.abs(valid_wc[isort[ichannel]]/np.max(np.abs(valid_wc))))\n",
        "\n",
        "        ax = axs[i, Nimg - 1]  # Rightmost axis in the row\n",
        "        if ichannel < 0: ichannel = len(valid_wc) + ichannel\n",
        "        ax.text(1.2, 0.5, f'channel {ichannel+1}', transform=ax.transAxes,\n",
        "                verticalalignment='center', fontsize=24, color=cmap(i))\n",
        "        if i == 0:\n",
        "            ax = axs[i, Nimg - 1]\n",
        "            ax.text(-5, 1.3, 'channel maximum stimuli', transform=ax.transAxes,\n",
        "                    verticalalignment='center', fontsize=24, color='black')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2dLmvsqpfJ3"
      },
      "source": [
        "---\n",
        "\n",
        "# 1. Data introduction â±ï¸ ~15â€¯min  \n",
        "\n",
        "In each recording there are ~5,000 neurons recorded simultaneously in mouse V1 responding to >30,000 natural and texture images.  \n",
        "\n",
        "In this section, we will load the image stimuli, neural response arrays, and associated metadata:\n",
        "\n",
        "- **`imgs`**: A collection of 68,000 images, each of shape `(66, 130)`.\n",
        "  - These include:\n",
        "    - 59,500 natural images presented at most one time during the recording session, used for training the model.\n",
        "    - 500 of these natural images presented 10 times during the recording session, used for testing the model.\n",
        "    - 8,000 texture images, used to assess texture-invariant representations (see more details on this in the paper).\n",
        "\n",
        "- **Neural recordings**: >30,000 images were shown to the mouse during the experiment, 500 of these images were repeated 10 times to create a test set with trial-averaged responses.\n",
        "  - **`spks`**: stimuli by neurons matrix\n",
        "  - **`istim_train`**: indices of the `imgs` matrix corresponding to each stimulus presentation in `spks`\n",
        "  - **`spks_test`**: stimuli by trials by neurons matrix\n",
        "  - **`istim_test`**: indices of the `imgs` matrix corresponding to each stimulus in `spks_test`\n",
        "  - **`ypos`, `xpos`**: vectors of length neurons with the position of each neuron in the recording\n",
        "\n",
        "Let's start by loading the images and neural responses using the provided utility functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E11hiA1YpfJ3",
        "outputId": "e29b7936-9228-4434-b220-1213d4de3a73"
      },
      "outputs": [],
      "source": [
        "from minimodel import data\n",
        "\n",
        "mouse_id = 3\n",
        "\n",
        "data_path = './data'\n",
        "weight_path = './checkpoints'\n",
        "\n",
        "# load images\n",
        "imgs = data.load_images(data_path, mouse_id, file=data.img_file_name[mouse_id])\n",
        "\n",
        "# load neural activity data\n",
        "fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
        "spks, istim_train, istim_test, xpos, ypos, spks_test = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
        "\n",
        "ypos, xpos = ypos / 0.5, xpos / 0.75 # convert to um, imaging at 0.5 x 0.75 um / pixel\n",
        "\n",
        "n_stim_train, n_neurons_all = spks.shape\n",
        "n_stim_test, n_reps, _ = spks_test.shape\n",
        "print('Loaded %d neurons w/ %d stimuli presented for training'%(n_neurons_all, n_stim_train))\n",
        "print('Loaded %d neurons w/ %d stimuli presented %d times for testing'%(n_neurons_all, n_stim_test, n_reps))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUgwdelSpfJ3"
      },
      "source": [
        "## 1.1 Visualize stimulus\n",
        "\n",
        "First, let's visualize the stimuli:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "eTX5bAs7pfJ3",
        "outputId": "ec6c5ad0-d895-4196-ba6f-21059f9269ae"
      },
      "outputs": [],
      "source": [
        "# plot example stimuli\n",
        "h_ = 3\n",
        "n_col = 8\n",
        "h, w  = imgs[0].shape  # height and width of stimulus\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "################################################################################\n",
        "# TODO for students: randomly sample example stimulus indices from the test set\n",
        "raise NotImplementedError(\"Student exercise: sample test stimuli to visualize\")\n",
        "################################################################################\n",
        "# Hint:\n",
        "# - Use np.random.choice to select h_ * n_col examples from istim_test\n",
        "# - Set replace=False to avoid duplicates\n",
        "example_stims = ...\n",
        "\n",
        "example_imgs = imgs[example_stims]\n",
        "\n",
        "fig, axs = plt.subplots(h_, n_col, figsize=(n_col, h_))\n",
        "for i in range(n_col * h_):\n",
        "  ax = axs[i // n_col, i % n_col]\n",
        "  ax.imshow(example_imgs[i], cmap='gray')\n",
        "  ax.axis('off')\n",
        "  ax.set_title(f'stim {example_stims[i]}')\n",
        "fig.suptitle(f'stimulus size: {h} x {w}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv-mZAQxpfJ3"
      },
      "source": [
        "## 1.2 Visualize neural responses\n",
        "\n",
        "Next, we visualize the neural responses. The dataset contains neural recordings from approximately 5,000 neurons in mouse V1. We will sort the neurons by their correlation for visualization and bin across neurons, using our algorithm *rastermap* (this will take ~1 min).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "OWa5SbZOpfJ4",
        "outputId": "10187d3d-5b22-4c39-e7d8-f24a7bc1dad7"
      },
      "outputs": [],
      "source": [
        "from rastermap import Rastermap\n",
        "\n",
        "model = Rastermap(n_PCs=200, bin_size=25).fit(spks.T)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "################################################################################\n",
        "# TODO for students: visualize the embedding\n",
        "raise NotImplementedError(\"Student exercise: visualize rastermap embedding\")\n",
        "################################################################################\n",
        "# Hint:\n",
        "# - model.X_embedding has shape (n_super_neurons, n_stimuli)\n",
        "# - What do these dimensions represent?\n",
        "# - Try showing a subset of the embedding (e.g., first 1000 stimuli)\n",
        "plt.imshow(..., cmap='gray_r', vmin=0, vmax=2, aspect='auto')\n",
        "\n",
        "\n",
        "plt.title('Rastermap embedding of neural activity')\n",
        "plt.xlabel('stimulus id')\n",
        "plt.ylabel('\"super\" neurons (bins of 25 neurons)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAcVqzynpfJ4"
      },
      "source": [
        "We can see neurons which have diverse responses per stimulus, and then neurons that seem to not respond to the stimuli. We want to focus on modeling neurons which are driven by stimuli, so we will choose stimulus-responsive neurons in the next section, based on response reliability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU26pnFRpfJ4"
      },
      "source": [
        "---\n",
        "\n",
        "# 2. Response Reliability: fraction of explainable variance â±ï¸ ~20â€¯min  \n",
        "\n",
        "For the test set, we presented 500 images, each shown 10 times. These repeated stimulus presentations allow us to measure the trial-to-trial consistency of each neuron.\n",
        "\n",
        "Here we plot the activity for a single neuron across trials:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "6B7I_i2GpfJ4",
        "outputId": "90afaee9-66fb-403d-8821-e82ba741c709"
      },
      "outputs": [],
      "source": [
        "# check activity\n",
        "print('neural activity (test set):', spks_test.shape)\n",
        "nstim, nreps, n_neurons = spks_test.shape\n",
        "\n",
        "ineuron = 50\n",
        "plot_repeats(spks_test, ineuron)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10FjCvc_pfJ4"
      },
      "source": [
        "As in Willeke et al and Cadena et al, we can quantify the fraction of variance a neuron has related to the stimulus - the Fraction of Explainable Variance (FEV) - as\n",
        "\n",
        "$$FEV = \\frac{\\text{var}[Y] - \\sigma^2_{\\text{noise}}}{\\text{var}[Y]}.$$\n",
        "\n",
        "$Y$ is the matrix for a single neuron of stimuli $i$ by repeats $j$, $\\text{var}[Y]$ is the total variance across all images and repeats, and the noise across repeats $\\sigma^2_{\\text{noise}}$ is defined as\n",
        "\n",
        "$$\\sigma^2_{\\text{noise}} =\\mathbb{E}_i [\\text{var}_j[Y]] $$\n",
        "\n",
        "the average across stimuli of the per stimulus variance across repeats.\n",
        "\n",
        "For the example neuron above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pjrTlPQpfJ4",
        "outputId": "51402ed2-4b5e-408d-f226-607e387c2030"
      },
      "outputs": [],
      "source": [
        "sp = spks_test[:, :, ineuron]\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# TODO for students: calculate the total variance and noise variance for a single neuron\n",
        "raise NotImplementedError(\"Student exercise: compute total and noise variance for a single neuron, then FEV\")\n",
        "################################################################################\n",
        "# Hint:\n",
        "# - total variance: variance across all responses, ignoring trial structure\n",
        "# - noise variance: average variance across trials (per image)\n",
        "# - Use ddof=1 for an unbiased estimator of the variance\n",
        "# - Can you figure out why we use ddof=1 instead of 0?\n",
        "noise_var = ...\n",
        "total_var = ...\n",
        "\n",
        "fev = (total_var - noise_var) / total_var\n",
        "\n",
        "print(f'fraction of explainable variance: {fev:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAvvr3EBpfJ4"
      },
      "source": [
        "We can now compute this for each neuron as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "SkmEmRzopfJ4",
        "outputId": "87cfac5e-194a-4a46-f3f6-77716c98bd06"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# TODO for students: calculate the total and noise variance for all neurons\n",
        "raise NotImplementedError(\"Student exercise: compute FEV for all neurons\")\n",
        "################################################################################\n",
        "# Hint:\n",
        "# - spks_test has shape (n_stimuli, n_trials, n_neurons)\n",
        "# - Compute noise variance by averaging variance across trials for each stimulus\n",
        "# - Compute total variance across all responses per neuron\n",
        "# - Use ddof=1 to get an unbiased estimate\n",
        "noise_var = ...\n",
        "total_var = ...\n",
        "\n",
        "FEV_all = (total_var - noise_var) / total_var\n",
        "\n",
        "print(f'FEV average +/- std: {FEV_all.mean():.3f} +/- {FEV_all.std():.3f}')\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
        "ax.hist(FEV_all, bins=20, color='lightgray', edgecolor='black')\n",
        "ax.set_xlabel('FEV')\n",
        "ax.set_ylabel('# of neurons')\n",
        "ax.set_title('FEV Distribution')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epRSUYSkB1Sc"
      },
      "source": [
        "**Q: What's the difference between a neuron with high FEV and a neurons with low FEV? And what would influence the FEV of the neuron?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqMeSTcjpfJ5"
      },
      "source": [
        "In Du et al, we focused on neurons with FEV > 0.15. For this tutorial we will increase the threshold to 0.4 to reduce the number of neurons to work with and to just visualize the most reliable neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkg_TYYvpfJ5",
        "outputId": "1eb4eb8c-60d4-42c3-f60d-389138c67c1d"
      },
      "outputs": [],
      "source": [
        "igood = np.where(FEV_all > 0.4)[0]\n",
        "spks = spks[:, igood]\n",
        "spks_test = spks_test[:, :, igood]\n",
        "FEV = FEV_all[igood]\n",
        "ypos = ypos[igood]\n",
        "xpos = xpos[igood]\n",
        "n_neurons = len(igood)\n",
        "print(f'Number of neurons with FEV > 0.4: {n_neurons}')\n",
        "print(f'average FEV of these neurons: {FEV.mean():.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iB-2OhypfJ5"
      },
      "source": [
        "---\n",
        "\n",
        "# 3. Baseline: Linear Regression â±ï¸ ~30-40â€¯min  \n",
        "\n",
        "Before introducing nonlinear models, we establish a performance baseline using a linear regression model to predict neural responses $Y$ directly from image pixels $X$ with a matrix $A$, such that the prediction $\\hat{Y} = XA$. We will also visualize the linear receptive fields of each neuron, they are defined by $A$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0zlzgt0pfJ5"
      },
      "source": [
        "## 3.1 Fitting linear regression model\n",
        "\n",
        "We will fit $A$ such that the mean squared error between the neural responses and the prediction is minimized: $\\text{argmin}_A \\sum(Y - XA)^2$. The solution which minimizes the cost function is\n",
        "\n",
        "$$ A = (X_\\text{train}^\\top X_\\text{train})^{-1} (X_\\text{train}^\\top Y_\\text{train})$$\n",
        "\n",
        "where $X$ is stimuli by pixels and $Y$ is stimuli by neurons.\n",
        "\n",
        "(optional) can you derive this equation yourself by finding the minimum of the cost function, i.e. taking the derivative of the cost function and setting it to zero and solving for $A$?\n",
        "\n",
        "To regularize the linear regression (called ridge regression), we can add a $\\lambda$ term to the diagonal of the input covariance matrix before inversion:\n",
        "\n",
        "$$ A = (X_\\text{train}^\\top X_\\text{train} + \\lambda I)^{-1} (X_\\text{train}^\\top Y_\\text{train})$$\n",
        "\n",
        "\n",
        "For the linear regression, we will z-score the neural activity and subtract the mean pixel values across stimuli - this means that $Y$ and $X$ are mean zero and we do not need to predict a bias term. We will also bin the stimuli across pixels to reduce the size of $A$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yXywkW7pfJ5"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# TODO for students: normalize the data (z-score)\n",
        "\n",
        "raise NotImplementedError(\"Student exercise: normalize Y_train and Y_test using z-scoring\")\n",
        "################################################################################\n",
        "# Hint:\n",
        "# - Subtract the mean and divide by the std across stimuli for each neuron\n",
        "# - This ensures all neurons are on the same scale\n",
        "# Question: Why is z-scoring important for linear models?\n",
        "Y_train = spks.copy()\n",
        "Y_train -= ...\n",
        "Y_train /= ...\n",
        "\n",
        "Y_test = spks_test.copy()\n",
        "Y_test -= ...\n",
        "Y_test /= ...\n",
        "\n",
        "# downsample the images by binning the stimuli in X and Y\n",
        "b = 2\n",
        "X = imgs.copy()\n",
        "X = np.reshape(X[:, : (h//b) * b, : (w//b) * b], (X.shape[0], h//b, b, w//b, b))\n",
        "X = X.mean(axis=(2,4))\n",
        "\n",
        "# flatten pixels and subtract mean pixel values across stimuli\n",
        "X = X.reshape(X.shape[0], -1)\n",
        "X -= X.mean(axis=0)\n",
        "X_train = X[istim_train]\n",
        "X_test = X[istim_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3sToPYukmHX"
      },
      "source": [
        "Compute $A$ from $X_{\\text{train}}$ and $Y_{\\text{train}}$.\n",
        "\n",
        "(optional) you can try playing with the regularizer - how does this affect performance?\n",
        "\n",
        "(optional) try using the GPU to speed up the linear regression, with the matrix multiplications on the GPU and by using `torch.linalg.solve`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIFXDEmlpfJ5"
      },
      "outputs": [],
      "source": [
        "# regularized linear regression from stimuli to neural responses\n",
        "################################################################################\n",
        "# TODO for students: calculate the covariance matrices XtX and XtY\n",
        "raise NotImplementedError(\"Student exercise: compute XtX and XtY for linear regression\")\n",
        "################################################################################\n",
        "# Hint:\n",
        "# - XtX is the stimulus covariance matrix (Xáµ€X)\n",
        "# - XtY is the stimulus-response covariance matrix (Xáµ€Y)\n",
        "# - X_train has shape (n_samples, n_pixels)\n",
        "# - Y_train has shape (n_samples, n_neurons)\n",
        "XtX = ...\n",
        "XtY = ...\n",
        "\n",
        "lam = 5e4 # regularizer\n",
        "\n",
        "# add a ridge regularizer to the linear regression with parameter \"lam\"\n",
        "XtX = XtX + lam * np.eye(X_train.shape[1])\n",
        "\n",
        "# regression matrix\n",
        "A = np.linalg.solve(XtX, XtY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azJaR6IHpfJ5"
      },
      "source": [
        "## 3.2 Linear receptive fields\n",
        "\n",
        "We can now visualize some of the per-neuron regression matrices, the columns of $A$ - these are the linear receptive fields of the neurons:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "7-KsCyXIpfJ5",
        "outputId": "72d551f9-dcb4-4077-fedb-434e23f89090"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# TODO for students: what are we visualizing here?\n",
        "raise NotImplementedError(\"Student exercise: interpret the visualization of RFs\")\n",
        "################################################################################\n",
        "# Hint:\n",
        "# - A is the weight matrix from linear regression: (n_pixels Ã— n_neurons)\n",
        "# - Each column in A represents how one neuron weights each image pixel\n",
        "# - By reshaping A into 2D maps, we can visualize the spatial receptive field for each neuron\n",
        "# - These receptive fields show *where* in the image each neuron is most sensitive to\n",
        "# - What does this tell us about each neuronâ€™s tuning?\n",
        "RF = ...  # reshape to receptive fields\n",
        "\n",
        "h_, n_col = 3, 6  # number of rows and columns for plotting\n",
        "ineurons = np.random.choice(A.shape[1], n_col * h_, replace=False)\n",
        "fig, axs = plt.subplots(h_, n_col, figsize=(n_col * 2, h_ * 2))\n",
        "for i in range(n_col * h_):\n",
        "    ax = axs[i // n_col, i % n_col]\n",
        "    show_stimulus(RF[ineurons[i]], ax=ax, filters=True, vmax=3*RF.std(axis=(1,2)).mean())\n",
        "    ax.set_title(f'neuron {ineurons[i]}')\n",
        "    ax.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E58M0vClpfJ5"
      },
      "source": [
        "Each neuron receives input from a particular part of the visual field, as we can see with the receptive fields. The receptive field center for each neuron varies continuously across primary visual cortex (V1). We can visualize it for this recording from a small part of V1, we will compute the RF center by smoothing the absolute value of the RFs. Each pixel in the image is ~1 degree in size, so we will rescale by the binning size to convert to visual angle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "zb_WrwX4pfJ5",
        "outputId": "e11598df-fc9a-41c4-879f-b24312a632f1"
      },
      "outputs": [],
      "source": [
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "RF_sm = gaussian_filter(np.abs(RF.copy()), 2, axes=(1, 2))\n",
        "################################################################################\n",
        "# TODO for students: find the RF center for each neuron\n",
        "raise NotImplementedError(\"Student exercise: compute imax, ymax, xmax for RF center\")\n",
        "################################################################################\n",
        "# Hint:\n",
        "# - RF_sm has shape (n_neurons, height, width)\n",
        "# - First flatten each RF map and find the index of the max value (imax)\n",
        "# - Then convert imax into 2D coordinates using np.unravel_index\n",
        "# - RF_sm.shape[1:] gives the spatial (y, x) dimensions\n",
        "imax = ...\n",
        "ymax, xmax = ...\n",
        "\n",
        "# convert to degrees of visual angle\n",
        "ymax = b * ymax - 33  # convert to y-pos in visual field, 0 -> -20\n",
        "xmax = b * xmax - 135 # convert to x-pos in visual field, 0 -> -135\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
        "for i in range(2):\n",
        "    axs[i].scatter(ypos, xpos, c=xmax if i==0 else ymax,\n",
        "                   s=20, cmap='gist_ncar',\n",
        "                   vmin=-135 if i==0 else -40, vmax=5 if i==0 else 40)\n",
        "    plt.colorbar(axs[i].collections[0], ax=axs[i],\n",
        "                 label='azimuth (deg)' if i==0 else 'elevation (deg)')\n",
        "    axs[i].set_xlabel('x-pos (um)')\n",
        "    axs[i].set_ylabel('y-pos (um)')\n",
        "    axs[i].axis('equal')\n",
        "    axs[i].invert_xaxis()\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8cPWVKDpfJ6"
      },
      "source": [
        "We can see that the receptive field centers vary continuously over the recording area."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0of4v3WXpfJ6"
      },
      "source": [
        "## 3.3 Quantifying model performance\n",
        "\n",
        "Let's quantify how well this linear model captures the neural activity. For this we will calculate the prediction on held-out test data:\n",
        "\n",
        "$$ \\hat Y_\\text{test} = X_\\text{test} A .$$\n",
        "\n",
        "We will compute the variance explained of the predicted test responses to the held-out test images, which is the total variance minus the unexplained - or residual - variance. We will use all 10 repeats which is equivalent to averaging the residuals across these trials:\n",
        "\n",
        "$$ \\text{var. exp.} = \\text{var}[Y] - \\mathbb{E}_j [(Y - \\hat{y})^2]$$\n",
        "\n",
        "The fraction of variance explained is the ratio of the variance explained and the total variance:\n",
        "\n",
        "$$ \\text{FVE} = \\frac{\\text{var}[Y] - \\mathbb{E}_j [(Y - \\hat{y})^2]}{\\text{var}[Y]} = 1 - \\frac{\\mathbb{E}_j [(Y - \\hat{y})^2]}{\\text{var}[Y]} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mekto2ImpfJ6",
        "outputId": "33e5c55b-e443-47a2-9391-9f966d86f10c"
      },
      "outputs": [],
      "source": [
        "# prediction on test data\n",
        "Y_pred_linear = X_test @ A\n",
        "\n",
        "################################################################################\n",
        "# TODO for students: compute the residual variance and fraction of variance explained (FEV)\n",
        "raise NotImplementedError(\"Student exercise: compute residual variance and FEV\")\n",
        "################################################################################\n",
        "# Hint:\n",
        "# - Y_pred_linear has shape (n_images, n_neurons)\n",
        "# - Y_test has shape (n_images, n_trials, n_neurons)\n",
        "# - Broadcast Y_pred_linear across trials to match Y_test shape\n",
        "# - Compute residual variance: mean squared error between prediction and actual\n",
        "# - Then compute FEV: 1 - residual / total variance\n",
        "residual = ...\n",
        "varexp = ...\n",
        "\n",
        "# add back bias term (will make our predictions comparable with later sections)\n",
        "Y_pred_linear += spks.mean(axis=0)\n",
        "\n",
        "print(f'mean frac varexp (per trial) = {varexp.mean():.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chnjt1wopfJ6"
      },
      "source": [
        "But we do not expect to explain *all* of the variance with a model of the stimulus responses - we know neurons are modulated by other variables including behaviors like running. Thus, we want to compute the fraction of *explainable* variance explained by taking the ratio between the variance explained and the explainable variance:\n",
        "\n",
        "$$ FEVE = \\frac{\\text{var}[Y] - \\mathbb{E}_j [(Y - \\hat{\\mathbf{y}})^2]}{\\text{var}[Y] - \\sigma^2_{\\text{noise}}} = 1 - \\frac{\\mathbf{E}_j[(Y - \\hat{\\mathbf{y}})^2] - \\sigma^2_\\text{noise}}{\\text{var}[Y] - \\sigma^2_\\text{noise}}.$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "-Gn_2zMEpfJ6",
        "outputId": "65515cf4-69b5-4c05-b4b6-d2b7a38ea1b0"
      },
      "outputs": [],
      "source": [
        "total_var = Y_test.var(axis=(0,1), ddof=1)\n",
        "\n",
        "################################################################################\n",
        "# TODO for students: compute noise variance and FEVE\n",
        "raise NotImplementedError(\"Student exercise: compute noise_var and FEVE_linear\")\n",
        "################################################################################\n",
        "# Hint:\n",
        "# - noise_var: variance across trials (per stimulus), then average across stimuli\n",
        "# - Use ddof=1 for unbiased estimate\n",
        "noise_var = ...\n",
        "FEVE_linear = ...\n",
        "\n",
        "plt.figure(figsize=(4,3))\n",
        "plt.hist(FEVE_linear, bins=20, color='lightgray', edgecolor='black')\n",
        "plt.xlabel('FEVE')\n",
        "plt.ylabel('# of neurons')\n",
        "\n",
        "print(f'FEVE (average +/- std): {FEVE_linear.mean():.3f} +/- {FEVE_linear.std():.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFu3UGpHpfJ6"
      },
      "source": [
        "---\n",
        "\n",
        "# 4. Hand-Crafted CNN Filters â±ï¸ ~30-40â€¯min  \n",
        "\n",
        "We found that relatively simple CNNs can model neural responses well. In this section, we explain how convolutional layers work and build fixed filters (e.g., edge detectors), visualize their outputs, and correlate their activations with neural responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOIx20k7pfJ6"
      },
      "source": [
        "## 4.1 2D convolution operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehKYfxg0pfJ6"
      },
      "source": [
        "A 2D convolution is an integral of the product of a filter $f$ and an input image $I$ computed at various positions as the filter is slid across the input. The output of the convolution operation at position $(x,y)$ can be written as follows, where the filter $f$ is size $(2K+1, 2K+1)$:\n",
        "\n",
        "\\begin{equation}\n",
        "C(x,y) = \\sum_{k_x=-K}^{K} \\sum_{k_y=-K}^{K} f(k_x,k_y) I(x+k_x,y+k_y)\n",
        "\\end{equation}\n",
        "\n",
        "This **convolutional filter** is often called a **kernel**.\n",
        "\n",
        "Here is an illustration of a 2D convolution from this [article](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-convolution-neural-networks-e3f054dd5daa):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "53Z3Tg4lpfJ7",
        "outputId": "fbbf828a-bd2e-4a36-8538-e35ec0fcaf04"
      },
      "outputs": [],
      "source": [
        "# @markdown Execute this cell to view convolution gif\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(url='https://miro.medium.com/max/700/1*5BwZUqAqFFP5f3wKYQ6wJg.gif', width=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQlEXziipfJ7"
      },
      "source": [
        "## 4.2 2D convolutions in deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcF_5AhwpfJ7"
      },
      "source": [
        "A 2D convolutional layer has multiple output channels. Each output **channel** is the result of a 2D convolutional filter applied to the input. In the gif below, the input is in blue, the filter is in gray, and the output is in green. The number of units in the output channel depends on the *stride* you set. In the gif below, the stride is 1 because the input image is sampled at each position, a stride of 2 would mean skipping over input positions. In most applications, especially with small filter sizes, a stride of 1 is used.\n",
        "\n",
        "*Technical note*: if kernel size is odd and you set the padding to half the kernel size and the stride to 1 (as is shown below), you get a **channel** of units that is the same size as the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJKOKSvYVwir"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "AqwjdE0FpfJ7",
        "outputId": "71c9b3e5-18c6-48dd-ae8f-9cbfc6dcb43a"
      },
      "outputs": [],
      "source": [
        "# @markdown Execute this cell to view convolution gif\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(url='https://miro.medium.com/max/790/1*1okwhewf5KCtIPaFib4XaA.gif', width=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onRjtG1UpfJ7"
      },
      "source": [
        "When we fit the linear receptive fields of neurons, we fit a matrix of size number of pixels by number of neurons. This is equivalent to a fully connected - or linear layer - in a neural network.\n",
        "\n",
        "Convolutional layers are different from their fully connected counterparts in two ways:\n",
        "  * In a fully connected layer, each unit computes a weighted sum over all the input units. In a convolutional layer, on the other hand, each unit computes a weighted sum over only a small patch of the input, referred to as the unit's **receptive field**.\n",
        "  * In a fully connected layer, each unit uses its own independent set of weights to compute the weighted sum. In a convolutional layer, all the units (within the same channel) **share the same weights**. This set of shared weights is called the **convolutional filter or kernel**. The result of this computation is a convolution, where each unit has computed the same weighted sum over a different part of the input. This reduces the number of parameters in the network substantially.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/NeuromatchAcademy/course-content/blob/main/tutorials/static/weight-sharing.png?raw=true\" width=\"600\" />\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-Pi-onypfJ7"
      },
      "source": [
        "## 4.3 2D convolutions in Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWCvkOGopfJ7"
      },
      "source": [
        "Now let's implement 2D convolutional operations. We will use multiple convolutional channels and implement this operation efficiently using pytorch. A *layer* of convolutional channels can be implemented with one line of code using the PyTorch class `nn.Conv2d()`, which requires the following arguments for initialization (see full documentation [here](https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html)):\n",
        "  * $C^{in}$: the number of input channels\n",
        "  * $C^{out}$: the number of output channels (number of different convolutional filters)\n",
        "  * $K$: the size of the $C^{out}$ different convolutional filters\n",
        "\n",
        "When you run the network, you can input a stimulus of arbitrary size $(H^{in}, W^{in})$, but it needs to be shaped as a 4D input $(N, C^{in}, H^{in}, W^{in})$, where $N$ is the number of images. In our case, $C^{in}=1$ because there is only one color channel (our images are grayscale, but often $C^{in}=3$ in image processing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl3qN_9ypfJ7"
      },
      "outputs": [],
      "source": [
        "class ConvolutionalLayer(nn.Module):\n",
        "  \"\"\" One convolutional layer\n",
        "     Attributes: conv (nn.Conv2d): convolutional layer\n",
        "  \"\"\"\n",
        "  def __init__(self, c_in=1, c_out=8, K=7, filters=None):\n",
        "    \"\"\"Initialize layer\n",
        "\n",
        "    Args:\n",
        "        c_in: number of input stimulus channels\n",
        "        c_out: number of output convolutional channels\n",
        "        K: size of each convolutional filter\n",
        "        filters: (optional) initialize the convolutional weights\n",
        "\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(c_in, c_out, kernel_size=K, padding=K//2, stride=1)\n",
        "    if filters is not None:\n",
        "      self.conv.weight = nn.Parameter(filters)\n",
        "      self.conv.bias = nn.Parameter(torch.zeros((c_out,), dtype=torch.float32))\n",
        "\n",
        "  def forward(self, s):\n",
        "    \"\"\"Run stimulus through convolutional layer\n",
        "\n",
        "    Args:\n",
        "        s (torch.Tensor): n_stimuli x c_in x h x w tensor with stimuli\n",
        "\n",
        "    Returns:\n",
        "        (torch.Tensor): n_stimuli x c_out x h x w tensor with convolutional layer unit activations.\n",
        "\n",
        "    \"\"\"\n",
        "    a = self.conv(s)  # output of convolutional layer\n",
        "\n",
        "    return a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeP_XAcxpfJ7"
      },
      "source": [
        "See that `ConvolutionalLayer` takes as input `filters`. We have predesigned some filters that you can use by calling the `filters` function below. These are similar to filters we think are implemented in biological circuits such as the retina and the visual cortex. The first two are **center-surround** filters and the next six are **gabor** filters. Check out this [website](http://www.cns.nyu.edu/~david/courses/perception/lecturenotes/ganglion/ganglion.html) for more details on center-surround filters, and this [website](https://en.wikipedia.org/wiki/Gabor_filter) for more details on gabor filters, if you're interested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "W5Zvv6N1pfJ7",
        "outputId": "0f6dcdfa-e8b1-415d-f102-73db443a4a5b"
      },
      "outputs": [],
      "source": [
        "# make filters\n",
        "example_filters = filters(out_channels=8, K=25)\n",
        "\n",
        "# visualize filters\n",
        "fig, axs = plt.subplots(1, 8, figsize=(12,4))\n",
        "for i in range(len(example_filters)):\n",
        "    ax = axs[i]\n",
        "    show_stimulus(example_filters[i].squeeze(), ax=ax, filters=True)\n",
        "    ax.set_title(f'filter {i}')\n",
        "    ax.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Siq7kKuLpfJ7"
      },
      "source": [
        "We will now run the convolutional layer on our stimulus. We will apply the convolution filters on the natural images with size 66x130.\n",
        "\n",
        "Reminder, `nn.Conv2d` takes in a tensor of size $(N, C^{in}, H^{in}, W^{in}$) where $N$ is the number of stimuli, $C^{in}$ is the number of input channels, and $(H^{in}, W^{in})$ is the size of the stimulus. We will need to add the channel dimension to our stimulus, then input it to the convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xDKspzQpfJ8",
        "outputId": "5971a018-7341-4046-a3b5-f90a743e8a8d"
      },
      "outputs": [],
      "source": [
        "# apply the filters on the images\n",
        "# Stimulus parameters\n",
        "in_channels = 1  # how many input channels in our images\n",
        "nstim = 5\n",
        "\n",
        "# select stimuli\n",
        "np.random.seed(1)\n",
        "example_stims = np.random.choice(imgs.shape[0], nstim, replace=False)\n",
        "stimuli = torch.from_numpy(imgs[example_stims])\n",
        "stimuli = stimuli.unsqueeze(1)  # add channel dimension\n",
        "print('input stimuli shape:', stimuli.shape)\n",
        "\n",
        "# Convolution layer parameters\n",
        "K = 25  # filter size\n",
        "out_channels = 8  # how many convolutional channels to have in our layer\n",
        "example_filters = filters(out_channels, K)  # create filters to use\n",
        "\n",
        "\n",
        "################################################################################\n",
        "## TODO for students: create convolutional layer in pytorch\n",
        "# Complete and uncomment\n",
        "raise NotImplementedError(\"Student exercise: create convolutional layer\")\n",
        "################################################################################\n",
        "\n",
        "# Initialize conv layer and add weights from function filters\n",
        "# you need to specify :\n",
        "# * the number of input channels c_in\n",
        "# * the number of output channels c_out\n",
        "# * the filter size K\n",
        "convLayer = ConvolutionalLayer(..., filters=example_filters)\n",
        "\n",
        "\n",
        "convout = convLayer(stimuli)  # run stimuli through convolutional layer\n",
        "convout = convout.detach().cpu().numpy()  # convert to numpy\n",
        "print('convolutional output shape:', convout.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx31CUmPpfJ8"
      },
      "source": [
        "Next we visualize the outputs of the convolution. `convout` is a tensor of size $(N, C^{out}, H^{in}, W^{in})$ where $N$ is the number of examples and $C^{out}$ are the number of convolutional channels. It is the same size as the input because we used a stride of 1 and padding that is half the kernel size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "K63EwCDtpfJ8",
        "outputId": "07601b7b-6d66-4654-a519-5ca75c5c3f54"
      },
      "outputs": [],
      "source": [
        "plot_example_activations(stimuli, convout, example_filters,\n",
        "                         channels=np.arange(0, out_channels), vmax=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfP0Ns9YXWe4"
      },
      "source": [
        "**Q: Do you notice any patterns across the activations from the Gabor filters? What would happen if we summed the absolute value of the Gabor filters with the same orientation but different phase?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmqxw9VppfJ8"
      },
      "source": [
        "For each location (x, y) on the input image, one filter is applied to compute the output at that location. We can calculate the correlation of the values of each filter at each position with the neural responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txA6gBcmpfJ8",
        "outputId": "2dc2682b-f2f4-48e8-9924-d3bd6974e03b"
      },
      "outputs": [],
      "source": [
        "# get test images\n",
        "imgs_test = imgs[istim_test]\n",
        "print('test images shape:', imgs_test.shape)\n",
        "\n",
        "# average responses across trials\n",
        "spks_test_avg = spks_test.mean(axis=1)\n",
        "print('average test responses shape:', spks_test_avg.shape)\n",
        "\n",
        "# compute convolutional layer responses to all test images\n",
        "stimuli = torch.from_numpy(imgs_test).unsqueeze(1) # add channel dimension\n",
        "convout_test = convLayer(stimuli).detach().cpu().numpy()\n",
        "print('convolutional output shape:', convout_test.shape)\n",
        "\n",
        "# calculate correlation of convolutional layer responses with neural responses\n",
        "convout_test = convout_test.reshape(convout_test.shape[0], -1)\n",
        "\n",
        "# correlation is matrix multiplication of z-scored neural responses and z-scored convolutional layer responses\n",
        "spks_test_avg -= spks_test_avg.mean(axis=0)\n",
        "spks_test_avg /= spks_test_avg.std(axis=0)\n",
        "convout_test -= convout_test.mean(axis=0)\n",
        "convout_test /= convout_test.std(axis=0)\n",
        "\n",
        "################################################################################\n",
        "# TODO for students: calculate the correlation between the activity and \n",
        "# the features from the conv layer.\n",
        "raise NotImplementedError(\"Student exercise: compute correlation between activity and conv features\")\n",
        "################################################################################\n",
        "\n",
        "# Hint:\n",
        "# - spks_test_avg has shape (n_images, n_neurons)\n",
        "# - convout_test has shape (n_images, n_channels)\n",
        "# - You want a (n_neurons Ã— n_channels) correlation matrix\n",
        "# - Don't forget to divide by the number of images to normalize the dot product\n",
        "cc = ...\n",
        "\n",
        "# reshape correlation matrix to (n_neurons, out_channels, h, w)\n",
        "cc = cc.reshape(spks_test_avg.shape[1], out_channels, h, w)\n",
        "\n",
        "print('correlation shape:', cc.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQIcEi0bpfJ8"
      },
      "source": [
        "We can visualize the correlations with each convolutional channel at each position for a random set of neurons:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "dN9v-4P8pfJ8",
        "outputId": "f014a9f8-ca7c-4543-a28f-57c47e939699"
      },
      "outputs": [],
      "source": [
        "ineurons = np.random.choice(cc.shape[0], 5, replace=False)\n",
        "plot_correlation_maps(spks_test[:, :, ineurons], cc[ineurons], example_filters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKFjFiULpfJ8"
      },
      "source": [
        "We can see that neurons often have higher correlations with activations in a certain part of the image, across maps. This is consistent with what we saw with the linear receptive fields: neurons are receiving visual input from a subset of space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tAJvhuppfJ8"
      },
      "source": [
        "---\n",
        "\n",
        "# 5. One-layer CNN (learnable filters + readout layer) â±ï¸ ~1â€¯hr  \n",
        "\n",
        "Now we will build a model to predict the neural activity using a convolutional layer. On top of this convolutional layer we add a readout layer that learns the spatial pooling and feature weighting for each neuron. The convolutional layer will be the same for all neurons, and the readout will have specific weights for each neuron.\n",
        "\n",
        "<img src=\"https://github.com/MouseLand/minimodel/blob/main/figures/model_structure/1layermodel.png?raw=true\" \n",
        "     alt=\"one-layer model structure\" \n",
        "     height=\"300\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaFRSMMSpfJ8"
      },
      "source": [
        "## 5.1 Neuron-specific readout weights\n",
        "\n",
        "We can see that in the above example we have 8 x 66 x 130 activations in the output of the convolutional layer, which is 68,640 outputs in total. If we use a fully-connected linear layer here, then we will need to fit 68,640 parameters per neuron. Instead we make some simplifying assumptions about the complexity of the spatial and feature pooling that the neuron does.\n",
        "\n",
        "First, we assume that the neuron pools spatially with all positive pooling weights, and that these spatial pooling weights can be approximated by a rank-1 multiplication:\n",
        "\n",
        "$$ W_{yx} = \\mathbf{w}_y \\mathbf{w}_x^\\top$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "ANWLQjaXpfJ9",
        "outputId": "6c988833-dbe7-493f-8d1e-6d8cc9b23e5d"
      },
      "outputs": [],
      "source": [
        "## spatial pooling weights\n",
        "\n",
        "# make example spatial pooling weights from gaussian filters\n",
        "w_y, w_x = np.zeros(h), np.zeros(w)\n",
        "w_y[h//2] = 1\n",
        "w_x[w//2] = 1\n",
        "# can vary width of gaussians to change the size/shape\n",
        "sigma_y, sigma_x = 8, 12\n",
        "w_y = gaussian_filter(w_y, sigma_y)\n",
        "w_x = gaussian_filter(w_x, sigma_x)\n",
        "\n",
        "################################################################################\n",
        "# TODO for students: compute the outer product of w_y and w_x\n",
        "raise NotImplementedError(\"Student exercise: compute spatial pooling matrix W_yx\")\n",
        "################################################################################\n",
        "# Hint:\n",
        "# - Use np.outer to create a 2D spatial pooling matrix from 1D vertical and horizontal weights\n",
        "# - This simulates a rank-1 spatial readout for a neuron\n",
        "W_yx = ...\n",
        "\n",
        "plt.figure(figsize=(3,1))\n",
        "plt.imshow(W_yx, cmap='gray');\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXNoqBgFpfJ9"
      },
      "source": [
        "**Q: How does this rank-1 assumption restrict the shape of the spatial pooling weights?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-7Ya812pfJ9"
      },
      "source": [
        "The next assumption we make is that all features (channels) have the same spatial pooling in the readout, meaning we have one $W_{yx}$ for each neuron, that is combined with the feature weights per channel $\\mathbf{w}_c$, as an outer product of all three vectors:\n",
        "\n",
        "$$ W_{cyx} = [w^1_c (\\mathbf{w}_y \\mathbf{w}_x^\\top) \\, \\cdots \\, w^{N_c}_c (\\mathbf{w}_y \\mathbf{w}_x^\\top)] $$\n",
        "\n",
        "where $N_c$ is the number of output channels in the convolutional layer.\n",
        "\n",
        "In the example above, this reduces the number of parameters in each neuron's readout from $HWN_c$ (68,640) to $H + W + N_c$ (204)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfjjyt90pfJ9"
      },
      "source": [
        "## 5.2 Combining conv layer with readout\n",
        "\n",
        "We next put these two parts together into a single model, and fit the weights of the model directly to the neural activity. We will fit this model from scratch, not initializing the filters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8WDHI12pfJ9",
        "outputId": "e7d8c723-8fe5-4723-efa7-bdfb030938e8"
      },
      "outputs": [],
      "source": [
        "class PredModel(nn.Module):\n",
        "  \"\"\" Model to predict neural responses w/ 1 conv layer and neuron-specific readouts\n",
        "     Attributes:\n",
        "        conv (nn.Conv2d): convolutional layer\n",
        "        W_y (torch.Tensor): readout weights over height of image, shape (n_neurons, h)\n",
        "        W_x (torch.Tensor): readout weights over width of image, shape (n_neurons, w)\n",
        "        W_c (torch.Tensor): readout weights over convolutional channels, shape (n_neurons, c_out)\n",
        "  \"\"\"\n",
        "  def __init__(self, c_in=1, c_out=32, K=13, n_neurons=1000,\n",
        "                h_in=33, w_in=65):\n",
        "    \"\"\" Initialize model\n",
        "\n",
        "    Args:\n",
        "        c_in: number of input stimulus channels\n",
        "        c_out: number of output convolutional channels\n",
        "        K: size of each convolutional filter\n",
        "        n_neurons: number of neurons to predict\n",
        "        h_in: height of input stimulus\n",
        "        w_in: width of input stimulus\n",
        "        maxpool: whether to apply max pooling after convolutional layer\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(c_in, c_out, kernel_size=K, padding=K//2, stride=1)\n",
        "    self.activation = nn.ELU()  # activation function\n",
        "    self.Wy = nn.Parameter(0.01 * torch.randn((n_neurons, h_in)))  # readout weights over height\n",
        "    self.Wx = nn.Parameter(0.01 * torch.randn((n_neurons, w_in)))  # readout weights over width\n",
        "    self.Wc = nn.Parameter(0.01 * torch.randn((n_neurons, c_out)))  # readout weights over convolutional channels\n",
        "\n",
        "\n",
        "  def forward(self, s):\n",
        "    \"\"\"Run stimulus through convolutional layer and readout\n",
        "\n",
        "    Args:\n",
        "        s (torch.Tensor): n_stimuli x c_in x h x w tensor with stimuli\n",
        "\n",
        "    Returns:\n",
        "        (torch.Tensor): n_stimuli x n_neurons tensor with predicted neural responses\n",
        "\n",
        "    \"\"\"\n",
        "    a = self.conv(s)  # output of convolutional layer\n",
        "    a = self.activation(a)\n",
        "    \n",
        "    ################################################################################\n",
        "    # TODO for students: use torch.einsum to implement the outer-product readout\n",
        "    # Hint:\n",
        "    # - a has shape (n_stimuli, c_out, h, w): conv output\n",
        "    # - Wy, Wx, Wc have shapes (n_neurons, h), (n_neurons, w), (n_neurons, c_out)\n",
        "    # - Output should be (n_stimuli, n_neurons): prediction per stimulus per neuron\n",
        "    raise NotImplementedError(\"Student exercise: apply factorized spatial-channel readout using einsum\")\n",
        "    ################################################################################\n",
        "    pred = ...\n",
        "\n",
        "    pred = self.activation(pred)\n",
        "    pred += 1 + 1e-12 # shift prediction to be all positive for ELU\n",
        "\n",
        "    return pred\n",
        "\n",
        "\n",
        "# we will downsample the images again to speed up training\n",
        "b = 2\n",
        "model = PredModel(c_in=in_channels, c_out=out_channels, K=K, n_neurons=n_neurons,\n",
        "                  h_in=h//b, w_in=w//b)#.to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# print model parameters\n",
        "print('trainable parameters:')\n",
        "print([(p[0], p[1].shape) for p in model.named_parameters()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V8f-fhWpfJ9"
      },
      "source": [
        "## 5.3 Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULCc4jcRpfJ9"
      },
      "source": [
        "We will use the **AdamW** optimizer to modify our weights to reduce the loss function, which consists of iterating three steps.\n",
        "\n",
        "1. **Evaluate the loss** on the training data in batches\n",
        "```python\n",
        "Y_pred = model(X_batch)\n",
        "loss = loss_fn(Y_pred, Y_batch)\n",
        "```\n",
        "where `X_batch` are a batch of training images, and `Y_batch` are the neuron responses to a batch of training images (tensor of number of stimuli by number of neurons).\n",
        "\n",
        "We will use the Poisson loss introduced in the Day 1 tutorial:\n",
        "```python\n",
        "loss = (Y_pred - Y_batch * torch.log(Y_pred)).sum(axis=0).mean()\n",
        "```\n",
        "\n",
        "2. **Compute the gradient of the loss** with respect to each of the network weights, with the `.backward()` method of the loss `loss`. Note that the gradients of each parameter need to be cleared before calling `.backward()`, otherwise the gradients will accumulate across iterations, with `.zero_grad()`:\n",
        "```python\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "```\n",
        "\n",
        "3. **Update the network weights** by descending the gradient. In Pytorch, we can do this using built-in optimizers. We'll use the `optim.AdamW` optimizer (documentation [here](https://pytorch.org/docs/stable/optim.html#torch.optim.AdamW)) which updates parameters along the negative gradient, scaled by a learning rate and scaled by the moving average of the gradients, and with scaled momentum. To optimize *all* the parameters of a network `model` using a learning rate of .001, the optimizer would be initialized as follows\n",
        "  ```python\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.1)\n",
        "  ```\n",
        "  \n",
        "  We can also specify learning rates and weight decay values per parameter sets like:\n",
        "  ```python\n",
        "  optimizer = torch.optim.AdamW([{'params': model.conv.parameters(), 'weight_decay': 0.1},\n",
        "                                 {'params': [model.Wy, model.Wx], 'weight_decay': 1.0},\n",
        "                                 {'params': model.Wc, 'weight_decay': 0.1},\n",
        "                                  ], lr=0.001)\n",
        "  ```\n",
        "\n",
        "\n",
        "  After computing all the parameter gradients in step 2, we can then update each of these parameters using the `.step()` method of this optimizer,\n",
        "  ```python\n",
        "  optimizer.step()\n",
        "  ```\n",
        "\n",
        "This optimizer [AdamW](https://pytorch.org/docs/stable/optim.html#torch.optim.AdamW) includes `weight_decay` which is an L2 regularization of the weights (reduces the magnitude of the weights on each iteration). Regularization is quite useful with noisy neural data, as we also saw with the linear regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEEnmGRepfJ9"
      },
      "outputs": [],
      "source": [
        "## define train and test data\n",
        "\n",
        "# train and test images\n",
        "X_train = imgs[istim_train].copy()\n",
        "X_train = X_train.reshape(X_train.shape[0], h//b, b, w//b, b).mean(axis=(2,4))  # downsample by 2\n",
        "X_test = imgs[istim_test].copy()\n",
        "X_test = X_test.reshape(X_test.shape[0], h//b, b, w//b, b).mean(axis=(2,4))  # downsample by 2\n",
        "\n",
        "# normalize neural activity by standard deviation\n",
        "Y_train = spks.copy()\n",
        "Y_train /= spks.std(axis=0)  # divide by std neuron response\n",
        "Y_test = spks_test.copy()\n",
        "Y_test /= spks.std(axis=0)  # divide by std neuron response\n",
        "\n",
        "# use one repeat for test loss (so it is similar to training loss)\n",
        "Y_test0 = Y_test[:,0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry0LZsmwyn2E"
      },
      "source": [
        "Start training the model, then maybe get a coffee â˜• it will take around 2-3 min w/ 100 neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwNdqpV-pfJ9",
        "outputId": "389e6953-7dbd-473e-ed9e-4825dcfdc5df"
      },
      "outputs": [],
      "source": [
        "# put data on GPU (if data is too big, this should be done on each batch)\n",
        "Y_train_t = torch.from_numpy(Y_train).to(device)\n",
        "Y_test0_t = torch.from_numpy(Y_test0).to(device)\n",
        "X_train_t = torch.from_numpy(X_train).unsqueeze(1).to(device)\n",
        "X_test_t = torch.from_numpy(X_test).unsqueeze(1).to(device)\n",
        "\n",
        "# declare model here to reinitialize as needed before training\n",
        "nconv1 = 32 # we will have 32 trainable filters\n",
        "model = PredModel(c_in=1, c_out=nconv1, K=13, n_neurons=n_neurons).to(device)\n",
        "\n",
        "# initialize PyTorch AdamW optimizer\n",
        "learning_rate = 5e-3\n",
        "optimizer = torch.optim.AdamW([{'params': model.conv.parameters(), 'weight_decay': 0.1},\n",
        "                                {'params': [model.Wy, model.Wx], 'weight_decay': 1},\n",
        "                                {'params': model.Wc, 'weight_decay': 0.1},\n",
        "                                ], lr=learning_rate)\n",
        "\n",
        "# Loop over epochs\n",
        "n_epochs = 60\n",
        "batch_size = 100\n",
        "train_loss, test_loss = np.zeros(n_epochs), np.nan * np.zeros(n_epochs)\n",
        "nstim, nstim_test = X_train.shape[0], X_test.shape[0]\n",
        "\n",
        "tic = time.time()\n",
        "for iepoch in range(n_epochs):\n",
        "    model.train() # set model to training mode\n",
        "    iperm = np.random.permutation(nstim)  # random permutation of stimuli\n",
        "    if iepoch % 15 == 0 and iepoch > 0:\n",
        "        learning_rate /= 3 # reduce learning rate by factor of 3 every 15 epochs\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = learning_rate\n",
        "    for istart in range(0, nstim, batch_size):\n",
        "        X_batch = X_train_t[iperm[istart : istart + batch_size]]\n",
        "        Y_batch = Y_train_t[iperm[istart : istart + batch_size]]\n",
        "        # compute network output from inputs in train_data\n",
        "        Y_pred = model(X_batch)\n",
        "\n",
        "        # evaluate loss function using the possion loss\n",
        "        loss = (Y_pred - Y_batch * torch.log(Y_pred)).sum(axis=0).mean()\n",
        "\n",
        "        optimizer.zero_grad() # clear previous gradients\n",
        "        loss.backward() # compute gradients\n",
        "        optimizer.step() # update weights\n",
        "\n",
        "        train_loss[iepoch] += loss.item() # average loss over epoch\n",
        "\n",
        "        # clamp Wy and Wx to be non-negative\n",
        "        model.Wy.data.clamp_(0)\n",
        "        model.Wx.data.clamp_(0)\n",
        "\n",
        "    train_loss[iepoch] /= nstim\n",
        "\n",
        "    # evaluate test loss\n",
        "    model.eval()\n",
        "    test_loss[iepoch] = 0.0\n",
        "    Y_pred_test = np.zeros((nstim_test, n_neurons), 'float32')\n",
        "    with torch.no_grad():\n",
        "        for istart in range(0, nstim_test, batch_size):\n",
        "            X_batch = X_test_t[istart : istart + batch_size]\n",
        "            Y_batch = Y_test0_t[istart : istart + batch_size]\n",
        "            Y_pred = model(X_batch)\n",
        "            Y_pred_test[istart : istart + batch_size] = Y_pred.cpu().numpy()\n",
        "            test_loss[iepoch] += (Y_pred - Y_batch * torch.log(Y_pred)).sum(axis=0).mean()\n",
        "        test_loss[iepoch] /= nstim_test\n",
        "\n",
        "    print(f'epoch {iepoch}, train_loss = {train_loss[iepoch]:0.4f}, test_loss = {test_loss[iepoch]:0.4f}, LR = {learning_rate:.1e}, time {time.time()-tic:.2f}s')\n",
        "\n",
        "# remove to free GPU memory\n",
        "del Y_train_t, Y_test0_t, X_train_t, X_test_t\n",
        "\n",
        "# save model\n",
        "model_path = os.path.join(weight_path, 'conv_model.pth')\n",
        "torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLfvzF9xpfJ-"
      },
      "source": [
        "Quantify performance with variance explained and FEVE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "wmRWvmW9pfJ-",
        "outputId": "e04a5b57-3d61-4497-b9df-88730f3e543d"
      },
      "outputs": [],
      "source": [
        "Y_pred_conv1 = Y_pred_test\n",
        "\n",
        "# variance explained per neuron - across stimuli and repeats\n",
        "residual = ((Y_pred_conv1[:,np.newaxis] - Y_test)**2).mean(axis=(0,1))\n",
        "varexp = 1 - residual / Y_test.var(axis=(0,1))\n",
        "\n",
        "print(f'mean frac varexp (per trial) = {varexp.mean():.3f}')\n",
        "\n",
        "total_var = Y_test.var(axis=(0,1), ddof=1)\n",
        "noise_var = Y_test.var(axis=1, ddof=1).mean(axis=0)\n",
        "FEVE_conv1 = 1 - (residual - noise_var) / (total_var - noise_var)\n",
        "\n",
        "print(f'FEVE (average +/- std): {FEVE_conv1.mean():.3f} +/- {FEVE_conv1.std():.3f}')\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(8,3))\n",
        "axs[0].hist(FEVE_conv1, bins=20, color='lightgray', edgecolor='black')\n",
        "axs[0].set_xlabel('FEVE')\n",
        "axs[0].set_ylabel('# of neurons')\n",
        "\n",
        "# compare to linear model\n",
        "axs[1].scatter(FEVE_linear, FEVE_conv1, s=10, c='b', alpha=0.5)\n",
        "axs[1].scatter(FEVE_linear.mean(), FEVE_conv1.mean(), c='r', marker='*', s=100)\n",
        "axs[1].plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "axs[1].set_xlabel('linear model')\n",
        "axs[1].set_ylabel('conv+readout model')\n",
        "axs[1].axis('square')\n",
        "axs[1].set_title('FEVE comparison')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR0YP5MNpfJ-"
      },
      "source": [
        "We have more than doubled our FEVE now with this fairly simple model.\n",
        "\n",
        "(optional) try changing the number of convolutional filters in the first layer (`nconv1`), and quantify performance.\n",
        "\n",
        "(optional) remove the clamping of the readout weights - allowing them to be positive and negative, and quantify performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mye8zsJcpfJ-"
      },
      "source": [
        "## 5.4 Visualizing the model\n",
        "\n",
        "We have the fully-learned first convolutional layer and the readout learned for each neuron.\n",
        "\n",
        "First we will look at the convolutional filters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "k2dM6PETpfJ-",
        "outputId": "780819f1-fcd0-470c-a5f3-438641e9780d"
      },
      "outputs": [],
      "source": [
        "conv_filters = model.conv.weight.data.detach().cpu().numpy()\n",
        "\n",
        "rm_model = Rastermap(n_PCs=16, n_clusters=None).fit(conv_filters.reshape(conv_filters.shape[0], -1))\n",
        "isort_filt = rm_model.embedding[:,0].argsort()\n",
        "conv_filters = conv_filters[isort_filt]\n",
        "\n",
        "# visualize filters\n",
        "fig, axs = plt.subplots(4, 8, figsize=(12,6))\n",
        "for i in range(len(conv_filters)):\n",
        "    ax = axs[i // 8, i % 8]\n",
        "    show_stimulus(conv_filters[i].squeeze(), ax=ax, filters=True, vmax=5e-1)\n",
        "    ax.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhIMzLW-pfJ-"
      },
      "source": [
        "**Q: What sorts of features do you see in these filters?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKwHhAh1pfJ-"
      },
      "source": [
        "Now we'll visualize the readout weights for a subset of neurons. Here we plot  $W_{yx} = \\mathbf{w}_y \\mathbf{w}_x^\\top$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "At7_o3FKpfJ-",
        "outputId": "14c5a1a1-ae71-415e-ebbd-af4517c6922f"
      },
      "outputs": [],
      "source": [
        "Wy = model.Wy.data.detach().cpu().numpy()\n",
        "Wx = model.Wx.data.detach().cpu().numpy()\n",
        "fig, axs = plt.subplots(10, 10, figsize=(12, 8))\n",
        "for i in range(100):\n",
        "    ax = axs[i // 10, i % 10]\n",
        "    ax.imshow(np.outer(Wy[i], Wx[i]), cmap='gray', vmin=0, vmax=5e-3)\n",
        "    ax.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd5FlYbOpfJ-"
      },
      "source": [
        "Note that the spatial readout weights are all positive, forcing the model to perform a positive pooling over pixels across channels. The **sign and strength** of each channel's contribution are instead controlled by the **channel weights**, which can be either positive or negative.\n",
        "\n",
        "The channel weights are neurons by channels, we will sort the neurons and channels by rastermap for visualization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "msDqPWPbpfJ_",
        "outputId": "19f2d625-3b7d-4d8a-f267-db92353d0c5b"
      },
      "outputs": [],
      "source": [
        "Wc = model.Wc.data.detach().cpu().numpy()\n",
        "\n",
        "# sort neurons\n",
        "rm_model = Rastermap(n_PCs=16, n_clusters=50).fit(Wc)\n",
        "Wc_sort = Wc[rm_model.embedding[:, 0].argsort()]\n",
        "\n",
        "# sort by filters\n",
        "Wc_sort = Wc_sort[:, isort_filt]\n",
        "\n",
        "# visualize Wc and filters\n",
        "plot_Wc(Wc_sort, conv_filters, vmax_filt=0.5, vmax_Wc=0.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUOAr3E9pfJ_"
      },
      "source": [
        "We have sorted the neurons by the similarities of their $\\mathbf{w}_c$ vectors. Even so we can many different patterns of $\\mathbf{w}_c$ across the population, suggesting that there are many different combinations of convolutional channels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvMaorfcpfJ_"
      },
      "source": [
        "---\n",
        "\n",
        "# 6. Two-layer CNN (â€œFull Modelâ€ for all the recorded neurons) â±ï¸ ~20-30â€¯min  \n",
        "\n",
        "We found in [Du et al](https://www.nature.com/articles/s41467-025-61171-9) that a two-layer CNN (with 16 and 320 channels) is sufficient to match more complex architectures. Here, we build and train this full model and visualize the learned weights.\n",
        "\n",
        "<img src=\"https://github.com/MouseLand/minimodel/blob/main/figures/model_structure/fullmodel.png?raw=true\" \n",
        "     alt=\"fullmodel structure\" \n",
        "     height=\"300\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1s9YzPX48Am"
      },
      "source": [
        "Here we build the model, specifying the number of convolutional layers `nlayers`, and the number of output channels per layer `nconv1` and `nconv2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG_VB9gSpfJ_",
        "outputId": "f92f835d-2157-4e42-edd8-4eb352ba4208"
      },
      "outputs": [],
      "source": [
        "# build model\n",
        "from minimodel import model_builder\n",
        "nlayers = 2\n",
        "nconv1 = 16\n",
        "nconv2 = 320\n",
        "fullmodel, in_channels = model_builder.build_model(NN=n_neurons, n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2)\n",
        "model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels)\n",
        "\n",
        "model_path = os.path.join(weight_path, 'fullmodel', model_name)\n",
        "fullmodel = fullmodel.to(device)\n",
        "\n",
        "print(fullmodel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENRBX5B36Zd9"
      },
      "source": [
        "## 6.1 Load / train the model\n",
        "\n",
        "On the T4, the full training will take over 1 hour, so we will load the checkpoint by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wQr1QdkpfJ_",
        "outputId": "783d2f1b-f213-43bd-a7a5-92a6e3acd1ea"
      },
      "outputs": [],
      "source": [
        "# train model\n",
        "from minimodel import model_trainer\n",
        "\n",
        "do_train = False # whether or not to train the model\n",
        "if do_train:\n",
        "    # split into train and validation set\n",
        "    itrain, ival = data.split_train_val(istim_train, train_frac=0.9)\n",
        "    spks_val = torch.from_numpy(Y_train[ival]).to(device)\n",
        "    spks_train = torch.from_numpy(Y_train[itrain]).to(device)\n",
        "    img_train = torch.from_numpy(imgs[istim_train][itrain]).to(device).unsqueeze(1)\n",
        "    img_val = torch.from_numpy(imgs[istim_train][ival]).to(device).unsqueeze(1)\n",
        "    print(\"img sizes: [train], [val]\")\n",
        "    print(img_train.shape, img_val.shape)\n",
        "    best_state_dict = model_trainer.train(fullmodel, spks_train, spks_val, img_train, img_val, device=device)\n",
        "    torch.save(best_state_dict, model_path)\n",
        "    print('saved model', model_path)\n",
        "else:\n",
        "    fullmodel, in_channels = model_builder.build_model(NN=n_neurons_all, n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2)\n",
        "    fullmodel = fullmodel.to(device)\n",
        "\n",
        "fullmodel.load_state_dict(torch.load(model_path), strict=False)\n",
        "print('loaded model', model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p2SH3j2VFbh"
      },
      "source": [
        "Now we quantify the model performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "YYMNQnYDpfJ_",
        "outputId": "a1efea56-e3f4-4648-c9e1-a8861615b0e8"
      },
      "outputs": [],
      "source": [
        "# compute model FEVE\n",
        "img_test = torch.from_numpy(imgs[istim_test]).to(device).unsqueeze(1)\n",
        "test_pred = model_trainer.test_epoch(fullmodel, img_test)\n",
        "Y_pred_fullmodel = test_pred[:, igood]\n",
        "\n",
        "residual = ((Y_pred_fullmodel[:,np.newaxis] - Y_test)**2).mean(axis=(0,1))\n",
        "varexp = 1 - residual / Y_test.var(axis=(0,1))\n",
        "\n",
        "print(f'mean frac varexp (per trial) = {varexp.mean():.3f}')\n",
        "\n",
        "FEVE_fullmodel = 1 - (residual - noise_var) / (total_var - noise_var)\n",
        "\n",
        "print(f'FEVE (average +/- std): {FEVE_fullmodel.mean():.3f} +/- {FEVE_fullmodel.std():.3f}')\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(8,3))\n",
        "axs[0].hist(FEVE_fullmodel, bins=20, color='lightgray', edgecolor='black')\n",
        "axs[0].set_xlabel('FEVE')\n",
        "axs[0].set_ylabel('# of neurons')\n",
        "\n",
        "# compare to linear model\n",
        "axs[1].scatter(FEVE_conv1, FEVE_fullmodel, s=10, c='b', alpha=0.5)\n",
        "axs[1].scatter(FEVE_conv1.mean(), FEVE_fullmodel.mean(), c='r', marker='*', s=100)\n",
        "axs[1].plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "axs[1].set_ylabel('16-320+readout model')\n",
        "axs[1].set_xlabel('conv+readout model')\n",
        "axs[1].axis('square')\n",
        "axs[1].set_title('FEVE comparison')\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyJSQUAlT86a"
      },
      "source": [
        "We explain even more variance with this two-layer model.\n",
        "\n",
        "We can look at `Y_pred_linear`, `Y_pred_conv1` and `Y_pred_fullmodel` for a few example neurons - what differences do you see?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "zqAlDJq_bdWc",
        "outputId": "124d83b0-98f5-4ba0-883f-b799f64dedb2"
      },
      "outputs": [],
      "source": [
        "k = 0 # neuron index\n",
        "\n",
        "plot_predictions(Y_test, Y_pred_linear, Y_pred_conv1, Y_pred_fullmodel, k=k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbgUmyBdpfKA"
      },
      "source": [
        "## 6.2 Visualize conv1\n",
        "\n",
        "As in the previous model, we can easily visualize the first layer of convolutional filters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "YZGbG-QvpfKA",
        "outputId": "d86366f8-6934-40fd-f796-126f44d37327"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# TODO for students: try to find the conv1 weights name from the model\n",
        "# Hint:\n",
        "# - Print the fullmodel structure to inspect the module hierarchy\n",
        "# - Look for the first conv layer under fullmodel.core.features\n",
        "raise NotImplementedError(\"Student exercise: extract conv1 weights from fullmodel\")\n",
        "################################################################################\n",
        "\n",
        "conv1_w = ...  # get conv1 weights from fullmodel\n",
        "\n",
        "print('conv1_w shape:', conv1_w.shape)\n",
        "isort = [14,3,4,5,15,0,12,11,10,6,7,8,9,13,1,2]\n",
        "fig, ax = plt.subplots(4, 4, figsize=(4, 4))\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        idx = i * 4 + j\n",
        "        ax[i, j].imshow(conv1_w[isort[idx]], cmap='coolwarm', vmin=-0.2, vmax=0.2)\n",
        "        ax[i, j].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YwxfTTHe0Oz"
      },
      "source": [
        "**Q: What do these filters look like?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ph39wljpfKA"
      },
      "source": [
        "## 6.3 Visualize the conv2\n",
        "\n",
        "The second convolutional layer is a **depthwise-separable** convolutional layer, which consists of two types of operations:\n",
        "\n",
        "1. **1Ã—1 convolution**: Since the filter size is 1Ã—1, the weights are of size input channels by output channels. The operation is equivalent to a linear operation, where each (x, y) location is weighted and summed across the input channels to create each output channel. This means for our 16 input channels and 320 output channels, we have 16x320 weights.\n",
        "\n",
        "2. **Spatial-only convolution**: operates over the **local spatial neighborhood** of each pixel, separately for each input channel. This means for our 9x9 spatial filters and 320 input channels, we have 9x9x320 weights.\n",
        "\n",
        "Why this 1Ã—1 â†’ spatial conv structure?\n",
        "\n",
        "This structure still allows for combining both across features (1x1 conv) and across pixels (spatial conv) which reducing the total number of parameters and operations that the network needs to perform. This increases efficiency and reduces network size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI88hRcvpfKA",
        "outputId": "58d2294a-3338-476e-9b89-58143b193c94"
      },
      "outputs": [],
      "source": [
        "conv2_1x1 = fullmodel.core.features.layer1.ds_conv.in_depth_conv.weight.data.cpu().numpy().squeeze()\n",
        "print('conv2_1x1 shape:', conv2_1x1.shape)\n",
        "conv2_spatial = fullmodel.core.features.layer1.ds_conv.spatial_conv.weight.data.cpu().numpy().squeeze()\n",
        "print('conv2_spatial shape:', conv2_spatial.shape)\n",
        "conv2_out = fullmodel.core.features.layer1.ds_conv.out_depth_conv.weight.data.cpu().numpy().squeeze()\n",
        "print('conv2_out shape:', conv2_out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsMwtFwGpfKA"
      },
      "source": [
        "**Q: How does each channel in the second layer relate to the first layer? How much of a reduction in parameters does this depthwise separable convolution have versus a standard convolutional layer?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFTE6bhDpfKA"
      },
      "source": [
        "Let's visualize the 1x1 convolution. Here we plot the 1x1 convolution weights as the rectangle colors from blue to red with their corresponding input conv1 filter. We show these for a few example output channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "wPkIXxQypfKA",
        "outputId": "55450ba7-196a-4070-ff01-18d82ba34174"
      },
      "outputs": [],
      "source": [
        "# visualize 1x1 conv\n",
        "plot_conv2_1x1(conv1_w, conv2_1x1, ichannels=[127,242, 102,68,204,217,293])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn41qkWopfKA"
      },
      "source": [
        "Let's visualize the spatial filters to see what the model has learned in the second layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "zISXfNsepfKA",
        "outputId": "0888a29e-7435-4b25-b44b-a0cfcc9c2ba3"
      },
      "outputs": [],
      "source": [
        "ichannels = [19,275,205,58, 197,313,276,308,129, 20]\n",
        "fig, axs = plt.subplots(1, 10, figsize=(14, 2))\n",
        "for i in range(10):\n",
        "    ax = axs[i]\n",
        "    ax.imshow(conv2_spatial[ichannels[i]], cmap='RdBu_r', vmin=-0.2, vmax=0.2)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f'channel {ichannels[i]}', fontsize=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf6M6pqjhFze"
      },
      "source": [
        "**Q: What sorts of filters do you see?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "johxneURpfKB"
      },
      "source": [
        "## 6.4 Visualize the readout\n",
        "\n",
        "As before, each neuron has a set of readout weights consisting of three vectors $\\mathbf{w}_y$, $\\mathbf{w}_x$, and $\\mathbf{w}_c$, which are combined as an outer product.\n",
        "\n",
        "We again visualize $W_{yx} = \\mathbf{w}_y \\mathbf{w}_x^\\top$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "qcwXwPVcpfKB",
        "outputId": "ffad42b4-a544-477a-c709-6690b4486fbf"
      },
      "outputs": [],
      "source": [
        "Wx = fullmodel.readout.Wx.detach().cpu().numpy()[igood]\n",
        "Wy = fullmodel.readout.Wy.detach().cpu().numpy()[igood]\n",
        "\n",
        "fig, axs = plt.subplots(10, 10, figsize=(12, 8))\n",
        "for i in range(100):\n",
        "    ax = axs[i // 10, i % 10]\n",
        "    ax.imshow(np.outer(Wy[i], Wx[i]), cmap='gray', vmin=0, vmax=5e-3)\n",
        "    ax.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqeTZFPmVfBc"
      },
      "source": [
        "**Q: How do these $W_{yx}$ matrices compare to the ones that were optimal for the one-layer model?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_892JKXopfKB"
      },
      "source": [
        "\n",
        "Now, letâ€™s visualize the channel weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "Gd6-oVYkpfKB",
        "outputId": "ed1a1747-b028-49c9-c779-f5c1cc7851a2"
      },
      "outputs": [],
      "source": [
        "Wc = fullmodel.readout.Wc.data.detach().cpu().numpy().squeeze()[igood]\n",
        "\n",
        "# sort neurons\n",
        "rm_model = Rastermap(n_PCs=32, n_clusters=50).fit(Wc)\n",
        "Wc_sort = Wc[rm_model.embedding[:, 0].argsort()]\n",
        "\n",
        "# sort by filters\n",
        "rm_model = Rastermap(n_PCs=32, n_clusters=50).fit(Wc_sort.T)\n",
        "Wc_sort = Wc_sort[:, rm_model.embedding[:, 0].argsort()]\n",
        "\n",
        "\n",
        "# visualize Wc and filters\n",
        "plot_Wc(Wc_sort, None, vmax_filt=0.5, vmax_Wc=0.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qZb2D8ApfKB"
      },
      "source": [
        "From $\\mathbf{w}_c$ we can observe that neurons have non-zero weights for many of the channels, using most of the 320 channels, making it hard to interpret the feature selectivity of a single neuron. Also, the neurons each have different patterns of $\\mathbf{w}_c$, meaning they are combining these channels in diverse ways, creating a high-dimensional space of image responses.\n",
        "\n",
        "[insert the fullmodel and minimodel structure]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWKwLieapfKB"
      },
      "source": [
        "**Q: Does the neuron have to use all channels? Or it can use a subset of channels?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qHW_dVzpfKB"
      },
      "source": [
        "---\n",
        "\n",
        "# 7. Minimodel per Neuron â±ï¸ ~45â€¯min  \n",
        "\n",
        "It appears that the neurons are quite diverse in their feature selectivity. Perhaps we can fit single neuron models which have a smaller second convolutional layer with features that are specific for each neuron. We call these \"minimodels\".\n",
        "\n",
        "<img src=\"https://github.com/MouseLand/minimodel/blob/main/figures/model_structure/minimodel.png?raw=true\" \n",
        "     alt=\"minimodel structure\" \n",
        "     height=\"300\"/>\n",
        "     \n",
        "Let's build a simple minimodel for one neuron. First we choose a single neuron (can try different neurons).\n",
        "\n",
        "To obtain optimal performance, we split our training set into training and validation data, and use the validation data assess performance across training. The best-performing model on the validation data is saved.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9DUfuADMeP4",
        "outputId": "abcb260b-5ecf-4e6e-bef7-ae02bb37da20"
      },
      "outputs": [],
      "source": [
        "# choose specific neuron in Y_train\n",
        "ineuron = 150\n",
        "ineur = [ineuron]\n",
        "\n",
        "# split neural data into train and val\n",
        "itrain, ival = data.split_train_val(istim_train, train_frac=0.9)\n",
        "\n",
        "# put data on GPU (if data is too big, this should be done on each batch)\n",
        "Y_train_t = torch.from_numpy(Y_train[itrain][:, ineur]).to(device)\n",
        "Y_val_t = torch.from_numpy(Y_train[ival][:, ineur]).to(device)\n",
        "Y_test0_t = torch.from_numpy(Y_test0[:, ineur]).to(device)\n",
        "X_train_t = torch.from_numpy(X_train[itrain]).unsqueeze(1).to(device)\n",
        "X_val_t = torch.from_numpy(X_train[ival]).unsqueeze(1).to(device)\n",
        "X_test_t = torch.from_numpy(X_test).unsqueeze(1).to(device)\n",
        "\n",
        "print('train images shape:', X_train_t.shape)\n",
        "print('val images shape:', X_val_t.shape)\n",
        "print('test images shape:', X_test_t.shape)\n",
        "print('train responses shape:', Y_train_t.shape)\n",
        "print('val responses shape:', Y_val_t.shape)\n",
        "print('test responses shape:', Y_test0_t.shape)\n",
        "\n",
        "Ly, Lx = X_train_t.shape[2], X_train_t.shape[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxgMAk-2oWxz"
      },
      "source": [
        "## 7.1 Build minimodel\n",
        "\n",
        "Here we build the minimodel with two convolutional layers (`nlayers = 2`), with 16 channels in the first conv layer (`nconv1`) as before, and we reduce the second convolutional layer to 64 output channels (`nconv2`). We have an additional regularizer on $\\mathbf{w}_c$ in this case, called the Hoyer-Square regularizer, which is a differentiable approximation of an L0 regularizer. We control the strength of this regularizer with `hs_readout`: increasing this value will increase the sparsity of the $\\mathbf{w}_c$ weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5q3N3gUt9kR",
        "outputId": "a3784b50-4479-46b1-f5e2-d1c69c2541af"
      },
      "outputs": [],
      "source": [
        "# build model\n",
        "from minimodel import model_builder\n",
        "nlayers = 2\n",
        "nconv1 = 16\n",
        "nconv2 = 64\n",
        "\n",
        "hs_readout = 0.03\n",
        "\n",
        "model, in_channels = model_builder.build_model(NN=1, n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, Wc_coef=0.2, input_Ly=Ly, input_Lx=Lx, kernel_size=[13,5])\n",
        "\n",
        "# name model according to structure\n",
        "model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id],\n",
        "                                             ineuron=ineur[0], n_layers=nlayers, in_channels=in_channels, hs_readout=hs_readout)\n",
        "model_path = os.path.join(weight_path, model_name)\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owD9MV_TpXy6"
      },
      "source": [
        "## 7.2 Train minimodel\n",
        "\n",
        "We will train the minimodel, with this [function](https://github.com/MouseLand/minimodel/blob/a917a4abd9c799ad4709c561e9587c1b8a41d03f/minimodel/model_trainer.py#L75). We initialize the first layer convolutional filters with the full model filters, and we turn off gradient computation for these filters - keeping their weights fixed during training.\n",
        "\n",
        "To speed up training, we will use the downsampled images as in previous sections. The full model was trained on the full images so we need to downsample the `conv1` filters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAM-1n-_M-OR",
        "outputId": "d0dec0e2-3844-4e56-c4e4-fb04d1175285"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "# Extract and downsample conv1 filters from the full model\n",
        "full_conv1 = fullmodel.core.features.layer0.conv.weight.data.cpu().numpy()\n",
        "print(\"Original shape:\", full_conv1.shape)\n",
        "\n",
        "ks = 13  # target kernel size\n",
        "conv1_w = np.array([\n",
        "    cv2.resize(filt[0], (ks, ks))[None, ...]\n",
        "    for filt in full_conv1\n",
        "])\n",
        "\n",
        "print(\"Downsampled shape:\", conv1_w.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzl4KTnrjNpA"
      },
      "source": [
        "Train the model, will take < 1 min."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6FO0UYtt9kR",
        "outputId": "d2497566-5b81-4877-ab08-b59cf9ee77b0"
      },
      "outputs": [],
      "source": [
        "# train model\n",
        "from minimodel import model_trainer\n",
        "if not os.path.exists(model_path):\n",
        "    ################################################################################\n",
        "    # TODO for students: initialize conv1 filters from the full model\n",
        "    # and prevent them from being updated during training\n",
        "    raise NotImplementedError(\"Student exercise: initialize and freeze conv1 filters\")\n",
        "    ################################################################################\n",
        "    # Hint:\n",
        "    # - Assign conv1_w to the conv1 weight of the model\n",
        "    # - Then set requires_grad = False for that same parameter\n",
        "    ... = torch.from_numpy(conv1_w).to(device, dtype=torch.float)\n",
        "    ... = False\n",
        "\n",
        "    # train the minimodel\n",
        "    best_state_dict = model_trainer.train(model, Y_train_t, Y_val_t, X_train_t, X_val_t, device=device, l2_readout=0.2, hs_readout=hs_readout)\n",
        "    torch.save(best_state_dict, model_path)\n",
        "    print('saved model', model_path)\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "print('loaded model', model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGdF_DlvqhMU"
      },
      "source": [
        "Now we quantify performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR7EKFTkt9kS",
        "outputId": "8a83d3d0-cdca-4976-d8cc-25b1f667ccb7"
      },
      "outputs": [],
      "source": [
        "# compute model FEVE\n",
        "test_pred = model_trainer.test_epoch(model, X_test_t)\n",
        "Y_pred_minimodel = test_pred.copy()\n",
        "\n",
        "residual = ((Y_pred_minimodel[:,np.newaxis] - Y_test[:,:,ineur])**2).mean(axis=(0,1))\n",
        "varexp = 1 - residual / Y_test[:,:,ineur].var(axis=(0,1))\n",
        "\n",
        "print(f'frac varexp (per trial) = {varexp.mean():.3f}')\n",
        "\n",
        "feve_minimodel = 1 - (residual - noise_var[ineur]) / (total_var[ineur] - noise_var[ineur])\n",
        "\n",
        "print(f'neuron {ineur[0]}, FEVE minimodel = {feve_minimodel.squeeze():.3f}, FEVE fullmodel = {FEVE_fullmodel[ineur].squeeze():.3f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9KzCKdUpfKC"
      },
      "source": [
        "## 7.3 Visualize conv2 in minimodel\n",
        "\n",
        "In the minimodel, we are reusing the conv1 from the fullmodel, so let's visualize the filters in conv2. Recall that we applied a sparsity penalty to the channel readout weights ($\\mathbf{w}_c$), encouraging the model to rely on only a small subset of channels. So we first will see which conv2 have large $\\mathbf{w}_c$ weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "H7kKkWCPpfKD",
        "outputId": "ea35a3b5-ac88-4a83-fedf-1a0e128b27f0"
      },
      "outputs": [],
      "source": [
        "Wc = model.readout.Wc.cpu().detach().numpy().squeeze()\n",
        "print('Wc shape:', Wc.shape)\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
        "ax.plot(np.sort(Wc), linewidth=2)\n",
        "ax.set_title(f'Wc for neuron {ineuron}')\n",
        "# set right and top spines invisible\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm5zhCr9pfKD"
      },
      "source": [
        "We can see that some channels now have *Wc* = 0, meaning the neuron completely ignores these channels when computing its response.\n",
        "\n",
        "**Q: why there are some channels have *Wc* = 0, and what if we want more channels to have *Wc* = 0? how to do that?**\n",
        "\n",
        "We will visualize the channels with large weights defined by a threshold on `np.abs(Wc)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3Sv6MFppfKD",
        "outputId": "f3ce4785-59c8-4814-ccc7-d027f7f9061c"
      },
      "outputs": [],
      "source": [
        "# only visualize the channels that contribute significantly to the readout\n",
        "top_channels = np.where(np.abs(Wc) > 0.3)[0]\n",
        "print(f'top channels: {len(top_channels)} / {len(Wc)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAqUIs2IucrH"
      },
      "source": [
        "We will visualize the top conv2 filters, as in the full model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "oFyBqQ1iusfv",
        "outputId": "cc6956fe-8bdf-4945-e7c3-48736f535406"
      },
      "outputs": [],
      "source": [
        "conv2_1x1 = model.core.features.layer1.ds_conv.in_depth_conv.weight.data.cpu().numpy().squeeze()\n",
        "plot_conv2(conv1_w.squeeze(), conv2_1x1, ichannels=top_channels, Wc=Wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUEFlonepfKD"
      },
      "source": [
        "**Q: Based on the visualization, what kind of features does the neuron seem to be mainly responding to?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEckmvPzpfKD"
      },
      "source": [
        "---\n",
        "\n",
        "# 8. Preferred Stimuli via Activation Maximization â±ï¸ ~30â€¯min  \n",
        "\n",
        "To better understand neuronal selectivityâ€”and how it relates to the selectivity of individual conv2 channelsâ€”we find the input images that **maximize the activation** of either a specific neuron or a specific feature channel.  \n",
        "\n",
        "These images reveal the visual patterns that individual neurons are most responsive to, and show how the features captured by each conv2 channel contribute to a neuron's selectivity.\n",
        "\n",
        "First, let's get the responses of each model neuron to the input images:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "Vb1mpmLSpfKD",
        "outputId": "5310ef0c-dd24-4193-a73f-58cd2058ce23"
      },
      "outputs": [],
      "source": [
        "# get the model predictions on the train images\n",
        "neuron_activity_model = model_trainer.test_epoch(model, X_train_t)\n",
        "neuron_activity_model = neuron_activity_model.squeeze()\n",
        "print('neuron_activity_model shape:', neuron_activity_model.shape)\n",
        "\n",
        "################################################################################\n",
        "# TODO for students: sort the stimuli based on predicted activity\n",
        "raise NotImplementedError(\"Student exercise: sort stimuli by model activity\")\n",
        "################################################################################\n",
        "# Hint:\n",
        "# - Use np.argsort to rank stimuli by model-predicted activity\n",
        "# - Reverse the order to show highest activity first\n",
        "prediction_isort = ...\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
        "ax.plot(np.arange(len(prediction_isort)), neuron_activity_model[prediction_isort], 'k', label='model')\n",
        "ax.set_xlabel('stimulus index (sorted by activity)')\n",
        "ax.set_ylabel('neuron activity')\n",
        "ax.set_title(f'Neuron {ineuron} activity')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXGTa482bhsB"
      },
      "source": [
        "Now, let's compute the activation of each conv2 channel across the input images.  \n",
        "We apply the spatial readout weights (*Wxy*) to pool each channel's feature map into a single scalar valueâ€”summarizing the channel's overall response to each image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utnQdJGtpfKD",
        "outputId": "25db2777-3da8-44a1-b87b-444be930b14c"
      },
      "outputs": [],
      "source": [
        "Nimgs_unique = X_train_t.shape[0]\n",
        "model.eval()\n",
        "batch_size = 160\n",
        "channel_activity = np.zeros((Nimgs_unique, nconv2))\n",
        "for i in range(0, Nimgs_unique, batch_size):\n",
        "    images = X_train_t[i:i+batch_size]\n",
        "    conv2_fv = model.core(images)\n",
        "\n",
        "    ################################################################################\n",
        "    # TODO for students: compute the activity of each conv2 channel after spatial pooling\n",
        "    raise NotImplementedError(\"Student exercise: apply spatial pooling to conv2 feature maps\")\n",
        "    ################################################################################\n",
        "    # Hint:\n",
        "    # - Use torch.einsum to apply Wx and Wy to the spatial dimensions of conv2 features\n",
        "    # - conv2_fv shape: (batch, channels, height, width)\n",
        "    # - Wx, Wy shape: (n_neurons, width) and (n_neurons, height)\n",
        "    # - Result should be (batch, channels)\n",
        "    wxy_fv = ...\n",
        "    \n",
        "    channel_activity[i:i+batch_size] = wxy_fv\n",
        "print('conv2 channel activity: ', channel_activity.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "c0RQNlPvpfKE",
        "outputId": "b1950192-6d50-4e20-b846-cac190ace3a7"
      },
      "outputs": [],
      "source": [
        "# visualize the sorted activity of each channel\n",
        "channel_activity_valid = channel_activity[:, top_channels]\n",
        "nchan = len(top_channels)\n",
        "cmap = plt.get_cmap('plasma', 9)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(3,3))\n",
        "for i in range(8):\n",
        "    ic = i if i<6 else -(8-i)\n",
        "    sorted_fv = np.sort(channel_activity_valid[:, ic])[::-1]\n",
        "    ic = i+1 if i<6 else nchan+1-(8-i)\n",
        "    ax.plot(sorted_fv[:200], color=cmap(i), label=f'channel {ic}')\n",
        "    if i ==7:\n",
        "        ax.text(0.5, 0.9-6*0.05, f'channel K', color=cmap(i), transform=ax.transAxes)\n",
        "    elif i<4:\n",
        "        ax.text(0.5, 0.9-i*0.05, f'channel {ic}', color=cmap(i), transform=ax.transAxes)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.set_xlabel('Stimulus (sorted)')\n",
        "ax.set_ylabel('Activity')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_R9PspBeapz"
      },
      "outputs": [],
      "source": [
        "# sort the activity of images in each channel\n",
        "channel_activity_isort = np.argsort(-channel_activity, axis=0)\n",
        "channel_activity_isort = channel_activity_isort[:, top_channels]\n",
        "# only take the top 8 images for each channel\n",
        "channel_activity_isort = channel_activity_isort[:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyqMqWkadnar",
        "outputId": "f86f5737-2d42-452d-dee7-1d7816baa333"
      },
      "outputs": [],
      "source": [
        "# generate a mask on the image based on the model Wxy\n",
        "from minimodel.utils import get_image_mask\n",
        "Ly, Lx = X_train_t.shape[-2:]\n",
        "img_mask = get_image_mask(model, Ly=Ly, Lx=Lx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "N1G3h_qIpfKE",
        "outputId": "a6a42c8b-21df-426f-a934-16680ff26b39"
      },
      "outputs": [],
      "source": [
        "valid_wc = Wc[top_channels]\n",
        "plot_maxstim(valid_wc, X_train_t, prediction_isort, channel_activity_isort, img_mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NX8zUSjpfKE"
      },
      "source": [
        "# 9. Wrap-Up & Discussion â±ï¸ ~15â€¯min  \n",
        "\n",
        "In this tutorial, we explored a range of models that predict neural responses in mouse V1, from simple linear regression to biologically inspired convolutional architectures. Along the way, we introduced key concepts such as receptive fields, spatial integration, and feature selectivityâ€”culminating in an interpretable, high-performing model.\n",
        "\n",
        "### Summary\n",
        "\n",
        "- **Even simple models can capture structure**:  \n",
        "  Linear regression and single-layer models with a factorized readout provided a strong baseline, with high interpretability.\n",
        "\n",
        "- **Factorized readouts offer insight into tuning**:  \n",
        "  The use of spatial weights ($\\mathbf{w}_y$, $\\mathbf{w}_x$) and channel weights $\\mathbf{w}_c$ enables us independently probe *where* a neuron samples from the visual field and *what* features it prefers.  \n",
        "\n",
        "- **Two-layer CNNs provide substantially better performance but are less interpretable**:  \n",
        "  As shown in *Du et al. (2024)*, a shallow model with just two convolutional layers achieves strong performance. However, the neurons are combinations of hundreds of features, making them difficult to interpret.\n",
        "\n",
        "- **Minimodels achieve equivalent performance with high interpretability**:  \n",
        "  By training a compact, neuron-specific readout on top of a shared convolutional backbone, the minimodel reproduces the performance of the full model with far fewer parametersâ€”making it both efficient and interpretable.\n",
        "  We could fit the minimodels with high sparsity in $\\mathbf{w}_c$, suggesting that most neurons only combine a smaller set of features.\n",
        "\n",
        "### Discussion questions\n",
        "\n",
        "- What are the advantages of shallow models over deeper networks in this context?\n",
        "- Could this modeling approach generalize to other brain areas or modalities?\n",
        "- What are the limits of this feedforward modelâ€”what kinds of neural dynamics are missing?\n",
        "\n",
        "### Suggested extensions\n",
        "\n",
        "If you're interested in pushing this work further, consider exploring:\n",
        "\n",
        "- How model performance changes with more or fewer conv filters\n",
        "- Using these models to study invariance using the natural texture stimuli\n",
        "\n",
        "\n",
        "---\n",
        "### Acknowledgments\n",
        "\n",
        "Some of this material was adapted from the Neuromatch Deep Learning Day [tutorials](https://compneuro.neuromatch.io/tutorials/W1D5_DeepLearning/student/W1D5_Tutorial1.html) by Jorge Menendez and Carsen Stringer."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6Q6kZPKupfJ1",
        "wbV4kdzApfJ2",
        "mSu0Sl02MjBH",
        "tS1j_dLZpfJ2"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
