{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MouseLand/course-materials/blob/main/behavior_encoding/tutorial_cajal_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAOR4DfXD7mP"
      },
      "source": [
        "# Behavioral encoding models of neural population activity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmRcAHFAD7mS"
      },
      "source": [
        "In this notebook, we will build several encoding models of neural activity based on the orofacial behaviors of mice.\n",
        "The encoding models are increasingly more complicated:\n",
        "1) linear regression from spatial keypoints\n",
        "2) linear regression from spatiotemporal keypoints\n",
        "3) nonlinear regression (i.e. neural networks) from spatiotemporal keypoints\n",
        "\n",
        "We will use rastermap to visualize the neural activity, and also to visualize the prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBcTyXlPD7mS"
      },
      "source": [
        "To keep the notebook interactive, there are exercises throughout\n",
        "\n",
        "* QUESTION MARKS: where ????? need to be replaced by a short equation, such as a variable or a function name.\n",
        "* Q: (discussion questions and quiz questions) have a short discussion with your colleague about this. At the end of each section we will discuss them as a group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dSYRzNhD7mT"
      },
      "source": [
        "**Recording info**: We will use a spontaneous activity recording from [Syeda et al, 2023](https://www.biorxiv.org/content/10.1101/2022.11.03.515121v1.abstract). We recorded 34,086 neurons from mouse sensorimotor cortex for 2+ hours using two-photon calcium imaging at a rate of 3.2Hz. FYI to make the download of the dataset faster, we are analyzing only the first half of the recording. During the recording, the mouse was free to run on an air floating ball, and we recorded the mouse face with a camera at a rate of 50Hz and tracked keypoints on the mouse face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GYfK1EBD7mV"
      },
      "source": [
        "If you are on google colab, select the GPU runtime:\n",
        "**Runtime > Change runtime type > Hardware accelerator = GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Setup\n",
        "\n",
        "Install libraries"
      ],
      "metadata": {
        "id": "05LIc2dHG_n7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reAInc6rD7mW"
      },
      "outputs": [],
      "source": [
        "!pip install rastermap\n",
        "# SUGGESTION: you can hide the ouput of a code cell after running it, by double-clicking on left of the output\n",
        "# SUGGESTION #2: you can instead run the pip install commands in a different anaconda prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions"
      ],
      "metadata": {
        "id": "pt4f--FOHn63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "\n",
        "# colors for the behaviors\n",
        "kp_colors = np.array([[0.55,0.55,0.55], [0.,0.,1],\n",
        "                      [0.8,0,0], [1.,0.4,0.2],\n",
        "                      [0,0.6,0.4], [0.2,1,0.5],\n",
        "                      ])\n",
        "pc_colors = plt.get_cmap('viridis')(np.linspace(0,0.9,8))\n",
        "\n",
        "def plot_pcs_run(Vsv, run, xmin, xmax):\n",
        "    fig = plt.figure(figsize=(10,6))\n",
        "    grid = plt.GridSpec(9, 1, figure=fig, hspace = 0.4)\n",
        "\n",
        "    # plot running speed\n",
        "    ax = plt.subplot(grid[0, 0])\n",
        "    ax.plot(run[xmin:xmax], color=kp_colors[0])\n",
        "    ax.set_xlim([0, xmax-xmin])\n",
        "    ax.axis('off')\n",
        "    ax.set_title('running speed', color=kp_colors[0])\n",
        "\n",
        "    for j in range(8):\n",
        "        ax = plt.subplot(grid[j+1])\n",
        "        ax.plot(Vsv[xmin:xmax, j], color=pc_colors[j])\n",
        "        ax.set_xlim([0, xmax-xmin])\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f'PC {j+1}', color=pc_colors[j])\n"
      ],
      "metadata": {
        "id": "OHzSqOp8Hpf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ld2snQLD7mY"
      },
      "source": [
        "Download data and assign variables from dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13Yv6cTND7mZ"
      },
      "outputs": [],
      "source": [
        "from rastermap import utils\n",
        "\n",
        "# download and load the spontaneous activity recording\n",
        "filename = utils.download_data(data_type='spont2')\n",
        "dat = np.load(filename)\n",
        "\n",
        "# unpack the dictionary into individual variables: spks = deconvolved fluorescence;\n",
        "spks = dat['spks']\n",
        "\n",
        "# we will z-score each neuron so that the activity is standard deviation 1 and mean 0 for each neuron\n",
        "spks = stats.zscore(spks, axis=1)\n",
        "\n",
        "# xpos and ypos and position of the neuron in the brain tissue\n",
        "xpos, ypos = dat['xpos'], dat['ypos']\n",
        "\n",
        "# tcam and tneural are times when the video and neural data were sampled, respectively\n",
        "tcam, tneural = dat['tcam'], dat['tneural']\n",
        "# convert to seconds\n",
        "tneural -= tcam[0]\n",
        "tcam -= tcam[0]\n",
        "tcam *= (24.0 * 60.0 * 60.0)\n",
        "tneural *= (24.0 * 60.0 * 60.0)\n",
        "\n",
        "# run is the running trace\n",
        "run = dat['run'][:spks.shape[1]]\n",
        "\n",
        "# beh are orofacial behavior, beh_names are the names of these orofacial behaviors\n",
        "beh, beh_names = dat['beh'], dat['beh_names']\n",
        "# we are using only first half of recording\n",
        "beh = beh[: np.nonzero(dat['tcam'] > dat['tneural'][-1])[0][0] + 50]\n",
        "tcam = tcam[: np.nonzero(dat['tcam'] > dat['tneural'][-1])[0][0] + 50]\n",
        "n_beh = len(beh_names)\n",
        "\n",
        "n_neurons, n_time = spks.shape\n",
        "\n",
        "print(f'{n_neurons} neurons by {n_time} timepoints')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaDA_0END7md"
      },
      "source": [
        "# 1. Visualizing neural activity with Rastermap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wceor5HpD7md"
      },
      "source": [
        "Let's first look at the positions of the neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkVqvYg3D7md"
      },
      "outputs": [],
      "source": [
        "# POSITIONS OF ALL NEURONS\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.scatter(ypos, xpos, s = 1)\n",
        "plt.xlabel('X position (um)')\n",
        "plt.ylabel('Y position (um)')\n",
        "plt.axis('square')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnzlF624D7me"
      },
      "source": [
        "Now let's run Rastermap. Rastermap re-arranges neurons in the raster plot based on similarity of activity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeyKl8WeD7me"
      },
      "outputs": [],
      "source": [
        "from rastermap import Rastermap\n",
        "\n",
        "### run rastermap\n",
        "nbin = 200 # number of neurons per superneuron\n",
        "rm_model = Rastermap(n_clusters=100, n_PCs=128, locality=0.6,\n",
        "                  time_lag_window=5, bin_size=nbin).fit(spks)\n",
        "cc_nodes = rm_model.cc.copy()\n",
        "\n",
        "# embedding is the embedding position for each neuron from rastermap\n",
        "embedding = rm_model.embedding[:,0]\n",
        "isort = embedding.argsort() # sorting of neurons by embedding\n",
        "\n",
        "# bin the sorted activity across neurons\n",
        "# (we average neurons instead of across trials to reduce variability)\n",
        "ndiv = (n_neurons // nbin) * nbin\n",
        "sn = spks[isort][:ndiv].reshape(-1, nbin, n_time).mean(axis=1)\n",
        "print('\"superneuron\" activity matrix', sn.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hf4zAawD7mg"
      },
      "source": [
        "Let's visualize the activity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHq1JESTD7mg"
      },
      "outputs": [],
      "source": [
        "# timepoints to visualize\n",
        "xmin = 0  # YOU can change this number\n",
        "xmax = xmin + 1000\n",
        "\n",
        "fig = plt.figure(figsize=(10,4), dpi=200)\n",
        "grid = plt.GridSpec(9, 10, figure=fig, wspace = 0.05, hspace = 0.3)\n",
        "\n",
        "# plot running speed\n",
        "ax = plt.subplot(grid[0, :-1])\n",
        "\n",
        "# REPLACE ????? with the running speed in the interval xmin to xmax\n",
        "ax.plot(run[xmin:xmax], color=kp_colors[0])\n",
        "ax.set_xlim([0, xmax-xmin])\n",
        "ax.axis('off')\n",
        "ax.set_title('running speed', color=kp_colors[0])\n",
        "\n",
        "# plot superneuron activity\n",
        "ax = plt.subplot(grid[1:, :-1])\n",
        "# REPLACE ????? with the sorted and binned neural activity in the interval xmin to xmax\n",
        "ax.imshow(sn[:, xmin:xmax], cmap='gray_r', vmin=0, vmax=0.8, aspect='auto')\n",
        "ax.set_xlabel('time')\n",
        "ax.set_ylabel('superneurons')\n",
        "\n",
        "ax = plt.subplot(grid[1:, -1])\n",
        "ax.imshow(np.arange(0, len(sn))[:,np.newaxis], cmap='gist_ncar', aspect='auto')\n",
        "ax.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: Approximately how many dimensions of neural activity do you think are in the data based on this plot?**"
      ],
      "metadata": {
        "id": "I7HAtzZyI_p9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_mfGS98D7mh"
      },
      "source": [
        "color the neurons by their position in the rastermap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkoZ6_M2D7mh"
      },
      "outputs": [],
      "source": [
        "# color the neurons by their position in the rastermap\n",
        "\n",
        "# POSITIONS OF ALL NEURONS\n",
        "plt.figure(figsize=(4, 4))\n",
        "\n",
        "# color the neurons in this plot with c = ?????, where ???? is the embedding position of each neuron from the Rastermap model\n",
        "plt.scatter(ypos, xpos, s=1, c=embedding, cmap='gist_ncar')\n",
        "plt.xlabel('X position (um)')\n",
        "plt.ylabel('Y position (um)')\n",
        "plt.axis('square');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JfbjqjYD7mi"
      },
      "source": [
        "# 2. Dimensionality reduction: what are the dominant patterns of activity?\n",
        "\n",
        "Let's compute the top PCs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuGaz-TaD7mi"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# this function returns the left singular vectors scaled by the singular values\n",
        "Vsv = TruncatedSVD(n_components = 128).fit_transform(spks.T)\n",
        "print('Vsv shape: ', Vsv.shape)\n",
        "\n",
        "# REPLACE ?????? to normalize the singular vectors to unit norm (across time)\n",
        "V = Vsv.copy() / (Vsv**2).sum(axis=0)**0.5\n",
        "\n",
        "# project the spiking data onto the singular vectors\n",
        "U = spks @ V\n",
        "\n",
        "# renormalize the neural projections\n",
        "U /= (U**2).sum(axis=0)**0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q:, answer A, B or C:**\n",
        "\n",
        "How to obtain the singular values of the data from the U matrix:\n",
        "- A) take the column norms of U\n",
        "- B) take the row norms of U\n",
        "- C) it cannot be obtained from U"
      ],
      "metadata": {
        "id": "Z_8-bGp_JRzZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omvMfeqND7mi"
      },
      "source": [
        "plot the PCs in time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgSKziOID7mi"
      },
      "outputs": [],
      "source": [
        "plot_pcs_run(Vsv, run, xmin, xmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaK2pjTfD7mj"
      },
      "source": [
        "We will predict the PCs from the behavior instead of the single neurons.\n",
        "\n",
        "**Q: Why might we use the PCs for predictions rather than the single neurons?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so7Xb-s3D7mj"
      },
      "source": [
        "# 3. Baseline model: linear regression\n",
        "\n",
        "We will first predict the neural PCs from the behavior using linear regression.\n",
        "\n",
        "The behavioral video is at 50Hz while the neural data is at 3.2 Hz, a difference in sampling rate of 17x.\n",
        "\n",
        "Here are the behaviors we tracked:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(n_beh, 1, figsize=(10,5))\n",
        "for j in range(n_beh):\n",
        "    axs[j].plot(beh[17*xmin : 17*xmax, j], color=kp_colors[j])\n",
        "    axs[j].set_xlim([0, 17*(xmax-xmin)])\n",
        "    axs[j].axis('off')\n",
        "    axs[j].set_title(beh_names[j], color=kp_colors[j])\n"
      ],
      "metadata": {
        "id": "pxTmm8wnLwzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSkZS_2lD7ml"
      },
      "source": [
        "Since the sampling rate and timing of neural activity and the behavior are different, we will downsample the behavior data to the timestamps of the neural activity.\n",
        "\n",
        "The easiest way to do this is with interpolation: we know when in time each behavior frame happened (`tcam`), and then we sample it at each time the neural activity happened (`tneural`). There are a lot of fast things going on in the behavior, so to get the average over timepoints at the neural activity time, we smooth the behavioral data first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pjsLYctD7ml"
      },
      "outputs": [],
      "source": [
        "from scipy.ndimage import gaussian_filter1d # here we import a smoothing function\n",
        "from scipy.interpolate import interp1d # importing an interpolation function\n",
        "\n",
        "# initialize empty matrix\n",
        "beh_ds = np.zeros((len(tneural), n_beh), 'float32')\n",
        "\n",
        "for j in range(n_beh):\n",
        "    # filter the data\n",
        "    # (smoothing scale proportional to difference in sampling rate)\n",
        "    bsmooth = gaussian_filter1d(beh[:,j], 50/3.2)\n",
        "\n",
        "    # interpolate\n",
        "    f = interp1d(tcam, bsmooth)\n",
        "\n",
        "    # REPLACE ?????\n",
        "    # look up the usage of interp1d to find out how to apply the function f at new times 'tneural'\n",
        "    beh_ds[:,j] = f(tneural)\n",
        "\n",
        "print(beh_ds.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the downsampled traces:"
      ],
      "metadata": {
        "id": "y-XfSwDCOr_F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlE0jvD9D7mm"
      },
      "outputs": [],
      "source": [
        "# plot the traces again\n",
        "fig, axs = plt.subplots(n_beh, 1, figsize=(10,5))\n",
        "for j in range(n_beh):\n",
        "    axs[j].plot(beh_ds[xmin:xmax, j], color=kp_colors[j])\n",
        "    axs[j].set_xlim([0, (xmax-xmin)])\n",
        "    axs[j].axis('off')\n",
        "    axs[j].set_title(beh_names[j], color=kp_colors[j])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdxIlgtWD7mm"
      },
      "source": [
        "Now, to do prediction, we have to do a train-test split. You always want to train your model on a subset of data and test its performance on another set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split behavioral times into train-test batches\n",
        "# * use interleaved segments *\n",
        "n_time_beh = beh.shape[0]\n",
        "n_segs = 20\n",
        "n_len  = n_time_beh / n_segs\n",
        "sinds = np.linspace(0, n_time_beh - n_len, n_segs).astype(int)\n",
        "itest = sinds[:,np.newaxis] + np.arange(0, n_len*0.25, 1, int)\n",
        "tpad = 50 # space in between segments\n",
        "itrain = sinds[:,np.newaxis] + np.arange(n_len*0.25 + tpad, n_len - tpad, 1, int)\n",
        "\n",
        "print(itrain.shape, itest.shape)\n",
        "\n",
        "# we will downsample into the neural timestamps for the linear regression\n",
        "f = interp1d(tcam, np.arange(len(tcam)))\n",
        "icam_best = np.round(f(tneural)).astype('int')\n",
        "\n",
        "# find indices of elements of itrain / itest in icam_best\n",
        "ineural_train = [np.nonzero(np.isin(icam_best, itrain[i]))[0] for i in range(n_segs)]\n",
        "ineural_test = [np.nonzero(np.isin(icam_best, itest[i]))[0] for i in range(n_segs)]\n",
        "\n",
        "itrain_ds = np.hstack(ineural_train)\n",
        "itest_ds = np.hstack(ineural_test)\n",
        "\n",
        "print(itrain_ds.shape, itest_ds.shape)"
      ],
      "metadata": {
        "id": "wblgl80rptla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: Why did we split the train/test into blocked segments rather than randomly interleaving time-points?**"
      ],
      "metadata": {
        "id": "97ZZDiULMQ2i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpG4MAuyD7mn"
      },
      "source": [
        "Use linear regression to perform the prediction, predict PCs $Y$ using behaviors $X$:\n",
        "\n",
        "$$ A = (X_\\text{train}^\\top X_\\text{train})^{-1} (X_\\text{train}^\\top Y_\\text{train})$$\n",
        "\n",
        "$X$ is time by behavioral components, $Y$ is time by neural components. If you want to regularize the linear regression:\n",
        "\n",
        "$$ A = (X_\\text{train}^\\top X_\\text{train} + \\lambda I)^{-1} (X_\\text{train}^\\top Y_\\text{train})$$\n",
        "\n",
        "Then the prediction on time points is:\n",
        "\n",
        "$$ \\hat Y_\\text{test} = X_\\text{test} A $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoZnu4DgD7mn"
      },
      "outputs": [],
      "source": [
        "## predict using behavior traces\n",
        "# regularized linear regression from behavior to neural PCs\n",
        "\n",
        "XtX = beh_ds[itrain_ds].T @ beh_ds[itrain_ds]\n",
        "XtY = beh_ds[itrain_ds].T @ Vsv[itrain_ds]\n",
        "lam = 1e1 # regularizer\n",
        "\n",
        "# REPLACE ?????\n",
        "# add a ridge regularizer to the linear regression with parameter 'lam'\n",
        "XtX = XtX + lam * np.eye(n_beh)\n",
        "\n",
        "# regression matrix\n",
        "A = np.linalg.solve(XtX, XtY)\n",
        "\n",
        "# prediction on test data, REPLACE ????? with behaviors on test points and the regression matrix\n",
        "Vpred_linear = beh_ds[itest_ds] @ A\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We quantify performance with fraction of variance explained (unnormalized). The fraction of variance explained (FVE) is the ratio of the variance explained and the total variance:\n",
        "\n",
        "$$ \\text{FVE} = \\frac{\\text{var}[Y] - \\text{var}[Y - \\hat{Y}]}{\\text{var}[Y]} = 1 - \\frac{\\text{var}[Y - \\hat{Y}]}{\\text{var}[Y]} $$\n",
        "\n",
        "We can compute this per PC $j$ as well:\n",
        "\n",
        "$$ \\text{FVE}_j = \\frac{\\text{var}[\\mathbf{y}_j] - \\text{var}[\\mathbf{y}_j - \\hat{\\mathbf{y}}_j]}{\\text{var}[\\mathbf{y}_j]} = 1 - \\frac{\\text{var}[\\mathbf{y}_j - \\hat{\\mathbf{y}}_j]}{\\text{var}[\\mathbf{y}_j]} $$\n"
      ],
      "metadata": {
        "id": "NkMjBrBSPIgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# overall varexp\n",
        "varexp_linear = 1 - (Vpred_linear - Vsv[itest_ds]).var() / (Vsv[itest_ds]).var()\n",
        "\n",
        "# variance explained per PC\n",
        "residual = (Vpred_linear - Vsv[itest_ds]).var(axis=0)\n",
        "varexp_PC = 1 - residual / Vsv[itest_ds].var(axis=0)\n",
        "\n",
        "print(f'overall fraction of variance explained = {varexp_linear : 0.3f}')"
      ],
      "metadata": {
        "id": "ZNqHh-4YPQKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMakP6CjD7mo"
      },
      "source": [
        "Plot PCs and prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gqzi3cgDD7mo"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(8, 1, figsize=(10, 10))\n",
        "for j in range(8):\n",
        "    ax = axs[j]\n",
        "    ax.plot(Vsv[itest_ds][xmin:xmax, j], color=pc_colors[j])\n",
        "    ax.plot(Vpred_linear[xmin:xmax, j], color='k', linestyle='--')\n",
        "    ax.set_xlim([0, xmax-xmin])\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f'PC {j+1}, FVE = {varexp_PC[j]:.2f}', color=pc_colors[j])\n",
        "    if j==0:\n",
        "        ax.legend(['PC', 'prediction'], loc='upper right')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: Why is the top PC predicted best? Does that mean something?**"
      ],
      "metadata": {
        "id": "nXasP3TmO_7V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MUrO4Z3D7mp"
      },
      "source": [
        "# 4. Convolutional neural network prediction\n",
        "\n",
        "There are finer temportal features in the behavioral features that we aren't capturing by smoothing and using the smoothed and downsampled traces.\n",
        "\n",
        "Instead we can learn the temporal features by using a 1D convolution layer with various filters -- called kernels.\n",
        "\n",
        "See below a nice illustration of a convolution from this [webpage](https://e2eml.school/convolution_one_d.html). This kernel is a gaussian, you can see how it smooths the data. But a neural network can learn whatever kernels help with prediction.\n",
        "\n",
        "![conv_gif](https://e2eml.school/images/conv1d/stride_1.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yGcaviDD7mp"
      },
      "source": [
        "Let's create a model to predict neural activity using convolutions.\n",
        "\n",
        "We will use `nn.Conv1d()` for the convolutional layer, which requires the following arguments for initialization (see full documentation [here](https://pytorch.org/docs/master/generated/torch.nn.Conv1d.html)):\n",
        "  * $C^{in}$: the number of input channels\n",
        "  * $C^{out}$: the number of output channels (number of different convolutional filters)\n",
        "  * $K$: the size of the $C^{out}$ different convolutional filters\n",
        "\n",
        "When you run the network, you can input a time series of arbitrary length $(L^{in})$, but it needs to be shaped as a 3D input $(N, C^{in}, L^{in})$, where $N$ is the number of batches.\n",
        "\n",
        "Note: as in the 2D convolutions we will want to use non-zero padding so that the output is the same size as the input (`padding = K//2`).\n",
        "\n",
        "We can initialize the convolutional filters with sines and cosines:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K = 201 # length of kernels\n",
        "theta = np.linspace(-np.pi, np.pi, K)\n",
        "freq = np.arange(1, 6)\n",
        "filters = np.vstack((np.sin(theta * freq[:, np.newaxis]),\n",
        "                     np.cos(theta * freq[:, np.newaxis])))\n",
        "plt.plot(filters.T);"
      ],
      "metadata": {
        "id": "VZIcB00xaPAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: Why initialize with sines and cosines for the temporal convolution layer?**"
      ],
      "metadata": {
        "id": "bl4H9-TvVjP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear prediction w/ 1D convolutions\n",
        "\n",
        "We will create a model with a linear layer, a 1D convolutional layer, and a linear output layer.\n",
        "\n",
        "To reduce the number of parameters, we will use a simplified version of a 1D convolutional layer. Each input to the convolutional layer will be filtered by the same filters. This enables us to compute many temporal functions of the behavioral inputs while minimizing the number of parameters.\n",
        "\n",
        "<img src=\"https://lh3.googleusercontent.com/d/11UDSc6uw51GYnmLhD75QLeHd6nwH_5E7\" alt=\"behavior to neural PC prediction net with one linear layer then a convolutional layer then another linear layer\" width=500></img>\n",
        "\n",
        "In the code, we have 20 inputs to the convolutional layer, and 10 filters - initialized as above - and so there are 120 outputs from the convolutional layer. The filters are each $K$ timepoints, and so it is $10\\times K$ parameters in this layer.\n",
        "\n",
        "**Q: If we were to implement this as a standard 1D convolutional layer with 20 inputs and 120 outputs, how many parameters would there be in the layer?**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r7F3zW7peer6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class PredModel(nn.Module):\n",
        "    \"\"\" Model to predict neural activity from behaviors w/ one conv layer\n",
        "      Attributes:\n",
        "          conv1x1 (nn.linear): linear layer\n",
        "          conv (nn.Conv1d): convolutional layer\n",
        "          activation (nn.ReLU): non-linearity\n",
        "          readout (nn.Sequential): linear layers + ReLUs (n_readout determines #)\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in=6, n_hidden=20, n_out=128, n_filters=10, K=201,\n",
        "                n_readout=1, nonlinear=False, filters=None):\n",
        "        \"\"\" Initialize network\n",
        "\n",
        "        Args:\n",
        "            n_in (int, optional): number of input behavior variables. Defaults to 6.\n",
        "            n_hidden (int, optional): number of combinations of behavior variables to be filtered by conv. Defaults to 20.\n",
        "            n_out (int, optional): number of output neurons / PCs. Defaults to 128.\n",
        "            n_filters (int, optional): number of different convolutional filters. Defaults to 10.\n",
        "            K: size of each convolutional filter\n",
        "            n_readout (int, optional): number of readout layers\n",
        "            nonlinear (bool, optional): whether to use nonlinearities. Defaults to False.\n",
        "            filters: (optional) initialize the convolutional weights\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # implement the first linear layer with a 1x1 conv layer (equivalent to a linear layer)\n",
        "        self.conv1x1 = nn.Conv1d(n_in, n_hidden, kernel_size=1)\n",
        "\n",
        "        # temporal convolution layer\n",
        "        # REPLACE ????? with conv1d parameters\n",
        "        self.conv = nn.Conv1d(1, n_filters, kernel_size=K, padding=K//2, stride=1)\n",
        "\n",
        "        # initialize filters\n",
        "        if filters is not None:\n",
        "            self.conv.weight = nn.Parameter(filters)\n",
        "            self.conv.bias = nn.Parameter(torch.zeros((n_filters,), dtype=torch.float32))\n",
        "\n",
        "        # optional activation\n",
        "        self.activation = nn.ReLU() if nonlinear else nn.Identity()\n",
        "\n",
        "        # readout from conv layers to neural activity\n",
        "        self.readout = nn.Sequential()\n",
        "        n_h = n_hidden * n_filters\n",
        "        # REPLACE ????? with linear layer parameters\n",
        "        self.readout.add_module('linear0', nn.Linear(n_h, n_out))\n",
        "\n",
        "        # add additional layers if n_readout > 1\n",
        "        for i in range(1, n_readout):\n",
        "            if nonlinear:\n",
        "                self.readout.add_module(f'relu{i-1}', nn.ReLU())\n",
        "            self.readout.add_module(f'linear{i}', nn.Linear(n_out, n_out))\n",
        "\n",
        "    def forward(self, x, verbose=False):\n",
        "        \"\"\"Run stimulus through convolutional layer\n",
        "\n",
        "        Args:\n",
        "            x (torch.tensor): n_batches x n_in x n_time\n",
        "            verbose (bool, optional): whether to print intermediate shapes. Defaults to False.\n",
        "\n",
        "        \"\"\"\n",
        "        h = self.conv1x1(x)\n",
        "        if verbose:\n",
        "            print('conv1x1 output: ', h.shape)\n",
        "\n",
        "        # apply conv layer to each output of the conv1x1\n",
        "        h = [self.conv(h[:, i:i+1]) for i in range(h.shape[1])]\n",
        "        h = torch.cat(h, axis=1)\n",
        "\n",
        "        # apply nonlinearity if nonlinear\n",
        "        h = self.activation(h)\n",
        "        if verbose:\n",
        "            print('conv output: ', h.shape)\n",
        "\n",
        "        # transpose convolutional output for the linear layer(s) in readout\n",
        "        h = h.transpose(2, 1)\n",
        "        if verbose:\n",
        "            print('input to readout: ', h.shape)\n",
        "\n",
        "        # readout\n",
        "        y = self.readout(h)\n",
        "        if verbose:\n",
        "            print('output: ', y.shape)\n",
        "\n",
        "        return y\n",
        "\n",
        "model = PredModel(filters=torch.from_numpy(filters).unsqueeze(1).float())\n",
        "print(model)\n",
        "\n",
        "beh_ex = torch.from_numpy(beh[:1000]).float().unsqueeze(0)\n",
        "print(beh_ex.shape)\n",
        "\n",
        "# reshape the behavior for the conv1x1 layer\n",
        "# REPLACE ????? with transpose\n",
        "beh_ex = beh_ex.transpose(2, 1)\n",
        "\n",
        "y = model(beh_ex, verbose=True)\n"
      ],
      "metadata": {
        "id": "6cpDrU38cIt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are running the network on the full, nondownsampled behavioral traces, the output of the network is at the timestamps of the behaviors `tcam`. We need to obtain the output at the `tneural` times, which is a subset of these timepoints. We will sample the output of the network at these timepoints to compute the loss function between the prediction and the true neural PC activity."
      ],
      "metadata": {
        "id": "6G_JQdgbBxfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "print('itrain = train times, shape: ', itrain.shape)\n",
        "print('itest = test times, shape: ', itest.shape)\n",
        "\n",
        "# put behavioral data on GPU\n",
        "X_train = torch.from_numpy(beh[itrain]).to(device)\n",
        "X_test = torch.from_numpy(beh[itest]).to(device)\n",
        "# transpose for conv1x1 layer\n",
        "X_train = X_train.transpose(2, 1)\n",
        "X_test = X_test.transpose(2, 1)\n",
        "\n",
        "## sample output of network at timepoints with neural activity\n",
        "\n",
        "# get indices of behavior with neural activity\n",
        "f = interp1d(tcam, np.arange(len(tcam)))\n",
        "icam_best = np.round(f(tneural)).astype('int')\n",
        "\n",
        "# obtain neural activity indices in each train batch\n",
        "in_train = np.isin(itrain, icam_best)\n",
        "isample_train = np.nonzero(in_train)\n",
        "isample_train = [isample_train[1][isample_train[0] == i] for i in range(n_segs)]\n",
        "\n",
        "# obtain neural activity indices in each test batch\n",
        "in_test = np.isin(itest, icam_best)\n",
        "isample_test = np.nonzero(in_test)\n",
        "isample_test = [isample_test[1][isample_test[0] == i] for i in range(n_segs)]\n",
        "\n",
        "# put on the GPU\n",
        "isample_train = [torch.from_numpy(isample_train[i]).to(device) for i in range(n_segs)]\n",
        "isample_test = [torch.from_numpy(isample_test[i]).to(device) for i in range(n_segs)]"
      ],
      "metadata": {
        "id": "dP72010ZrFPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: Why might we subsample the output of the network for predicting the neural activity rather than using another strategy? Are there any advantages/disadvantages?**"
      ],
      "metadata": {
        "id": "KbGA3asUW1pe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the timepoints of the neural activity in the training and test set from above `ineural_train` and `ineural_test`. We will use these timepoints and put the neural PCs on the GPU."
      ],
      "metadata": {
        "id": "U3wKf45iChj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = [torch.from_numpy(Vsv[ineural_train[i]]).to(device) for i in range(n_segs)]\n",
        "Y_test = [torch.from_numpy(Vsv[ineural_test[i]]).to(device) for i in range(n_segs)]\n",
        "Y_test_cpu = np.vstack([Vsv[ineural_test[i]] for i in range(n_segs)])"
      ],
      "metadata": {
        "id": "RycNMiC-O7pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss function computes the MSE (mean squared error) between the prediction and the true neural PC activity, and during optimization of the parameters we minimize this. We will add an another optional term to the loss function to impose smoothness on our convolutional filters. We compute smoothness as the squared difference between consecutive timepoints in the filter."
      ],
      "metadata": {
        "id": "hcxv2DalDkcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(y_pred, y_true, sm_penalty=0.1, cweight=None):\n",
        "    # mse loss\n",
        "    mse_loss = nn.MSELoss()\n",
        "    # REPLACE ?????\n",
        "    loss = mse_loss(y_pred, y_true)\n",
        "\n",
        "    # temporal conv smoothness penalty\n",
        "    if sm_penalty > 0 and cweight is not None:\n",
        "        smloss = sm_penalty * (torch.diff(cweight)**2).sum()\n",
        "        loss += smloss\n",
        "    return loss"
      ],
      "metadata": {
        "id": "fBZHcDQSKn7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now define the training function:"
      ],
      "metadata": {
        "id": "EbGZ6RfdJxcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, Y_train, isample_train,\n",
        "                X_test, Y_test, isample_test,\n",
        "                learning_rate = 5e-3, weight_decay = 1e-4,\n",
        "                sm_penalty = 0.1, n_epochs=250, batch_size=4,\n",
        "                n_period_init=100, n_period_next=50,\n",
        "                verbose=True):\n",
        "    \"\"\" train PredModel to predict neural activity\n",
        "\n",
        "        Args:\n",
        "            model (PredModel): network to train\n",
        "            X_train (torch.tensor): n_batches x n_in x n_time\n",
        "            Y_train (torch.tensor): n_batches x n_out x n_time\n",
        "            isample_train (list of torch.tensor): indices for neural activity samples in train set\n",
        "            X_test (torch.tensor): n_batches x n_in x n_time_test\n",
        "            Y_test (torch.tensor): n_batches x n_out x n_time_test\n",
        "            isample_test (list of torch.tensor): indices for neural activity samples in test set\n",
        "            learning_rate (float, optional): learning rate. Defaults to 5e-3.\n",
        "            weight_decay (float, optional): weight decay. Defaults to 1e-4.\n",
        "            sm_penalty (float, optional): temporal conv smoothness penalty. Defaults to 0.1.\n",
        "            n_epochs (int, optional): number of epochs. Defaults to 250.\n",
        "            batch_size (int, optional): batch size. Defaults to 4.\n",
        "            n_period_init (int, optional): number of epochs until first learning rate reduction. Defaults to 100.\n",
        "            n_period_next (int, optional): number of epochs until next learning rate reductions. Defaults to 50.\n",
        "            verbose (bool, optional): whether to print train loss / test loss. Defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            train_loss, test_loss, Y_pred_test\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize PyTorch AdamW optimizer\n",
        "    # REPLACE ????? with optimizer inputs\n",
        "    optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                                  weight_decay=weight_decay,\n",
        "                                  lr=learning_rate)\n",
        "\n",
        "    # epochs to reduce learning rate\n",
        "    epochs_per_period = [n_period_init]\n",
        "    while epochs_per_period[-1] < n_epochs:\n",
        "        epochs_per_period.append(epochs_per_period[-1] + n_period_next)\n",
        "\n",
        "    train_loss, test_loss = np.zeros(n_epochs), np.nan * np.zeros(n_epochs)\n",
        "    n_batches, n_batches_test = X_train.shape[0], X_test.shape[0]\n",
        "\n",
        "    # Loop over epochs\n",
        "    tic = time.time()\n",
        "    for iepoch in range(n_epochs):\n",
        "        model.train() # set model to training mode\n",
        "        iperm = np.random.permutation(n_batches)  # random permutation of batches\n",
        "        if iepoch in epochs_per_period and iepoch > 0:\n",
        "            learning_rate /= 3 # reduce learning rate by factor of 3 in each period\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = learning_rate\n",
        "        for istart in range(0, n_batches, batch_size):\n",
        "            inds = iperm[istart : istart + batch_size]\n",
        "            X_batch = X_train[inds]\n",
        "            Y_batch = torch.cat([Y_train[i] for i in inds], axis=0)\n",
        "\n",
        "            # compute network output from inputs in train_data\n",
        "            # REPLACE ????? with call to forward method of model\n",
        "            Y_pred = model(X_batch)\n",
        "\n",
        "            # sample at neural activity times\n",
        "            Y_pred = torch.cat([Y_pred[j, isample_train[i]] for j, i in enumerate(inds)], axis=0)\n",
        "\n",
        "            # evaluate loss function\n",
        "            # REPLACE ????? with inputs to loss function\n",
        "            loss = loss_fn(Y_pred, Y_batch, sm_penalty=sm_penalty,\n",
        "                          cweight=model.conv.weight)\n",
        "\n",
        "            optimizer.zero_grad() # clear previous gradients\n",
        "            loss.backward() # compute gradients\n",
        "            optimizer.step() # update weights\n",
        "\n",
        "            train_loss[iepoch] += loss.item() * len(inds) # average loss over epoch\n",
        "\n",
        "        train_loss[iepoch] /= n_batches\n",
        "\n",
        "        # evaluate test loss\n",
        "        if iepoch % 5 == 0:\n",
        "            model.eval()\n",
        "            test_loss[iepoch] = 0.0\n",
        "            Y_pred_test = []\n",
        "            with torch.no_grad():\n",
        "                for i in range(n_batches_test):\n",
        "                    X_batch = X_test[i : i+1]\n",
        "                    Y_batch = Y_test[i : i+1]\n",
        "                    Y_pred = model(X_batch)\n",
        "\n",
        "                    # sample at neural activity times\n",
        "                    Y_pred = Y_pred[0, isample_test[i]]\n",
        "                    Y_pred_test.append(Y_pred.cpu().numpy())\n",
        "\n",
        "                    # evaluate loss function\n",
        "                    test_loss[iepoch] += loss_fn(Y_pred, Y_batch[0], sm_penalty=sm_penalty,\n",
        "                                                  cweight=model.conv.weight)\n",
        "                test_loss[iepoch] /= n_batches_test\n",
        "                Y_pred_test = np.vstack(Y_pred_test)\n",
        "                varexp_nl = 1 - (Y_pred_test - Y_test_cpu).var() / (Y_test_cpu).var()\n",
        "\n",
        "            if verbose:\n",
        "                print(f'epoch {iepoch}, train_loss = {train_loss[iepoch]:.3f}, test_loss = {test_loss[iepoch]:.3f}, varexp = {varexp_nl:.3f}, LR = {learning_rate:.1e}, time {time.time()-tic:.2f}s')\n",
        "\n",
        "    return train_loss, test_loss, Y_pred_test\n"
      ],
      "metadata": {
        "id": "DxfCERwAkidi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first train the model without any nonlinearities (`nonlinear=False`). This means that we are fitting a form of 'reduced rank' regression, that is factorized across time and input variables."
      ],
      "metadata": {
        "id": "QexuL7DyOsfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PredModel(n_readout=1, nonlinear=False,\n",
        "                  filters=torch.from_numpy(filters).unsqueeze(1).float())\n",
        "device = torch.device('cuda')\n",
        "model.to(device)\n",
        "\n",
        "train_loss, test_loss, Y_pred_test = train_model(model, X_train, Y_train, isample_train,\n",
        "                                                 X_test, Y_test, isample_test,\n",
        "                                                 sm_penalty=0.1)\n"
      ],
      "metadata": {
        "id": "U-doZzF5OsGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the filters that the network learned, what timescales do you notice?\n",
        "\n"
      ],
      "metadata": {
        "id": "rRDC2MKWJs_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learned_filters = model.conv.weight.data.detach().cpu().numpy().squeeze()\n",
        "\n",
        "plt.plot(np.arange(-K//2, K//2) / 50, learned_filters.T);\n",
        "plt.xlabel('time (sec.)')"
      ],
      "metadata": {
        "id": "8N2RJiv-JMV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Q: What happens when we remove the smoothing penalty? Test this empirically.**"
      ],
      "metadata": {
        "id": "1_yTgAHRXf1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compute the variance explained overall, and the per-PC variance explained:"
      ],
      "metadata": {
        "id": "bDYhPmzYPayx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMEstYqbD7mr"
      },
      "outputs": [],
      "source": [
        "Vpred_conv = Y_pred_test.copy()\n",
        "\n",
        "# overall variance explained\n",
        "residual = ((Vpred_conv - Vsv[itest_ds])**2).sum()\n",
        "varexp_conv = 1 - residual / (Vsv[itest_ds]**2).sum()\n",
        "\n",
        "print(f'overall fraction of variance explained = {varexp_conv : 0.3f}')\n",
        "\n",
        "# variance explained per PC\n",
        "residual = ((Vpred_conv - Vsv[itest_ds])**2).sum(axis=0)\n",
        "varexp_PC_conv = 1 - residual / (Vsv[itest_ds]**2).sum(axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sozpMnmwD7ms"
      },
      "source": [
        "It looks like this fit better than the linear regression model which did not use temporal information. Let's see what the prediction looks like, it seems like the higher PCs are better captured especially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A77PDyg-D7ms"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 10))\n",
        "grid = plt.GridSpec(8, 1, figure=fig, hspace = 0.4)\n",
        "varexp_PC_conv = 1 - (Y_pred_test - Y_test_cpu).var(axis=0) / (Y_test_cpu).var(axis=0)\n",
        "for j in range(8):\n",
        "    ax = plt.subplot(grid[j])\n",
        "    ax.plot(Y_test_cpu[xmin:xmax, j], color=pc_colors[j])\n",
        "    ax.plot(Y_pred_test[xmin:xmax, j], color='k', linestyle='--')\n",
        "    ax.set_xlim([0, xmax-xmin])\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f'PC {j+1}, varexp = {varexp_PC_conv[j]:.2f}', color=pc_colors[j])\n",
        "    if j==0:\n",
        "        ax.legend(['PC', 'prediction'], loc='upper right')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcA0p4z8D7mt"
      },
      "source": [
        "## Receptive fields of superneurons\n",
        "\n",
        "We can use this linear model to estimate the receptive fields of the superneurons from the Rastermap.\n",
        "\n",
        "First we need to get the superneuron PC weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz9rsfb3D7mt"
      },
      "outputs": [],
      "source": [
        "# sort and bin PCs for maxstim estimation\n",
        "ndiv = (U.shape[0] // nbin) * nbin\n",
        "U_sn = U[isort][:ndiv].reshape(ndiv//nbin, nbin, -1).mean(axis=1)\n",
        "U_sn = torch.from_numpy(U_sn).to(device)\n",
        "\n",
        "print(U_sn.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's find the behavioral \"stimuli\" that maximize the activity of the superneurons. We will set up the stimuli as one per batch with size of `K`. We will maximize the activity of the superneuron at the middle time point `K//2`. As in maximizing stimuli in visual models, we will constrain the norm of the stimulus, resetting it each iteration."
      ],
      "metadata": {
        "id": "WJGDZ_MmWgVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ns = U_sn.shape[0]\n",
        "\n",
        "# each batch is a max stim\n",
        "RFs = 0.1 * torch.randn((ns, n_beh, K), device=device)\n",
        "RFs.requires_grad = True\n",
        "\n",
        "optimizer = torch.optim.Adam([RFs], lr=1e-1, weight_decay=0)\n",
        "n_iter = 200\n",
        "model.eval()\n",
        "for iter in range(n_iter):\n",
        "    losses = 0\n",
        "    # normalize stimuli\n",
        "    RFs_norm = RFs / (1e-3 + (RFs**2).mean(axis=(1,2), keepdims=True)**0.5)\n",
        "    y = model(RFs_norm)\n",
        "    y = y[:, K//2] # use middle time point\n",
        "\n",
        "    y = (y * U_sn).sum(axis=-1)\n",
        "\n",
        "    # REPLACE ????? with loss for max_stim ... what might we want to minimize?\n",
        "    loss = (-y).mean()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses+=loss.item()\n",
        "    if iter % 50 == 0 or iter == n_iter-1:\n",
        "        print(f'iter {iter}, loss = {losses : .3f}')\n",
        "\n",
        "# detach and convert to CPU\n",
        "RFs = RFs.detach().cpu().numpy()\n",
        "RFs /= (RFs**2).sum(axis=(1,2), keepdims=True)**0.5\n"
      ],
      "metadata": {
        "id": "JluWfzB5bEvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAf8HQ9oD7mu"
      },
      "source": [
        "Visualize a subset of receptive fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OK0NBPyD7mu"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(12,6), dpi=200)\n",
        "grid = plt.GridSpec(12, 21, figure=fig, wspace = 0.05, hspace = 0.0)\n",
        "\n",
        "vmax = 0.2\n",
        "ks = np.linspace(5, len(U_sn)-5, 12*3).astype('int')\n",
        "for i, k in enumerate(ks):\n",
        "    for j in range(n_beh):\n",
        "        ax = plt.subplot(grid[i%12, j + 6*(i//12) + (i//12)])\n",
        "        ax.plot(RFs[k, j], color=kp_colors[j])\n",
        "        ax.set_ylim([-vmax, vmax])\n",
        "        ax.axis('off')\n",
        "        if i==0:\n",
        "            ax.set_title(beh_names[j], color=kp_colors[j], rotation=45)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HjXu_H6D7mv"
      },
      "source": [
        "Visualize the receptive fields with the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoSbxVprD7mv"
      },
      "outputs": [],
      "source": [
        "# timepoints to visualize\n",
        "xmin = 0\n",
        "xmax = xmin + 1000\n",
        "\n",
        "fig = plt.figure(figsize=(12,6), dpi=200)\n",
        "grid = plt.GridSpec(9, 18, figure=fig, wspace = 0.35, hspace = 0.3)\n",
        "\n",
        "# plot running speed\n",
        "ax = plt.subplot(grid[0, :12])\n",
        "ax.plot(run[xmin:xmax], color=kp_colors[0])\n",
        "ax.set_xlim([0, xmax-xmin])\n",
        "ax.axis('off')\n",
        "ax.set_title('running speed', color=kp_colors[0])\n",
        "\n",
        "# plot superneuron activity\n",
        "ax = plt.subplot(grid[1:, :12])\n",
        "ax.imshow(sn[:, xmin:xmax], cmap='gray_r', vmin=0, vmax=0.8, aspect='auto')\n",
        "ax.set_xlabel('time')\n",
        "ax.set_ylabel('superneurons')\n",
        "\n",
        "for j in range(n_beh):\n",
        "    ax = plt.subplot(grid[1:, j+12])\n",
        "    ax.imshow(RFs[:,j], aspect='auto', vmin=-vmax, vmax=vmax, cmap='RdBu_r')\n",
        "    ax.axis('off')\n",
        "    ax.set_title(beh_names[j], color=kp_colors[j], rotation=45)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: What does a 'receptive field' from behavior mean? Is it the same as the receptive field of a sensory neuron, like a Gabor model? How is it similar / different?**"
      ],
      "metadata": {
        "id": "y3Dz4tXKV0QC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdbenL2cD7mw"
      },
      "source": [
        "## Nonlinear prediction w/ 1D convolutions\n",
        "\n",
        "We can put non-linearities in our neural network models to better model non-linear aspects of the data. We also add another layer to make it more complex. The network below is the default network from the Facemap paper.\n",
        "\n",
        "<img src=\"https://lh3.googleusercontent.com/d/1NluhnseWjZS7euHZhsfXil3rEkhVBRGx\" alt=\"behavior to neural PC prediction net with one linear layer then a convolutional layer then another linear layer\" width=500></img>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVrwE4M8D7mw"
      },
      "outputs": [],
      "source": [
        "model = PredModel(n_readout=2, nonlinear=True,\n",
        "                  filters=torch.from_numpy(filters).unsqueeze(1).float())\n",
        "device = torch.device('cuda')\n",
        "model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: how many layers are in this network?**\n",
        "* A: <=3\n",
        "* B: >3 but <=6\n",
        "* C: >6"
      ],
      "metadata": {
        "id": "yZdqmnKPigRT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A20d5G7JD7mx"
      },
      "source": [
        "Train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jmGYm21D7mw"
      },
      "outputs": [],
      "source": [
        "train_loss, test_loss, Y_pred_test = train_model(model, X_train, Y_train, isample_train,\n",
        "                                                 X_test, Y_test, isample_test,\n",
        "                                                 sm_penalty=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantify the performance of the model:"
      ],
      "metadata": {
        "id": "xfcjjDihi20w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDgmECVPD7mx"
      },
      "outputs": [],
      "source": [
        "Vpred_nl = Y_pred_test.copy()\n",
        "\n",
        "# overall variance explained\n",
        "residual = ((Vpred_nl - Vsv[itest_ds])**2).sum()\n",
        "varexp_nl = 1 - residual / (Vsv[itest_ds]**2).sum()\n",
        "\n",
        "print(f'overall fraction of variance explained = {varexp_nl : 0.3f}')\n",
        "\n",
        "# variance explained per PC\n",
        "residual = ((Vpred_nl - Vsv[itest_ds])**2).sum(axis=0)\n",
        "varexp_PC_nl = 1 - residual / (Vsv[itest_ds]**2).sum(axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsrcDT3SD7mx"
      },
      "source": [
        "This model did even better at fitting the neural activity! Visualize the prediction. Remember we need to project using the PCs into the neuron space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "579TURsvD7my"
      },
      "outputs": [],
      "source": [
        "sn_pred_nl = U_sn.cpu().numpy() @ Vpred_nl.T\n",
        "sn_pred_linear = U_sn.cpu().numpy() @ Vpred_linear.T\n",
        "\n",
        "fig = plt.figure(figsize=(12,14))\n",
        "grid = plt.GridSpec(20, 1, figure=fig, wspace = 0.35, hspace = 0.7)\n",
        "\n",
        "# plot running speed\n",
        "ax = plt.subplot(grid[0, 0])\n",
        "ax.plot(run[itest_ds][xmin:xmax], color=kp_colors[0])\n",
        "ax.set_xlim([0, xmax-xmin])\n",
        "ax.axis('off')\n",
        "ax.set_title('running speed', color=kp_colors[0])\n",
        "\n",
        "# plot superneuron activity\n",
        "ax = plt.subplot(grid[1:7, 0])\n",
        "ax.imshow(sn[:, itest_ds][:, xmin:xmax], cmap='gray_r', vmin=0, vmax=0.85, aspect='auto')\n",
        "ax.set_ylabel('superneurons')\n",
        "ax.set_xticks([])\n",
        "ax.set_title('neural activity')\n",
        "\n",
        "# plot superneuron prediction\n",
        "ax = plt.subplot(grid[7:14, 0])\n",
        "ax.imshow(sn_pred_nl[:, xmin:xmax], cmap='gray_r', vmin=0, vmax=0.85, aspect='auto')\n",
        "ax.set_ylabel('superneurons')\n",
        "ax.set_xticks([])\n",
        "ax.set_title('behavior prediction - nonlinear')\n",
        "\n",
        "\n",
        "ax = plt.subplot(grid[14:21, 0])\n",
        "ax.imshow(sn_pred_linear[:, xmin:xmax], cmap='gray_r', vmin=0, vmax=0.85, aspect='auto')\n",
        "ax.set_xlabel('time')\n",
        "ax.set_ylabel('superneurons')\n",
        "ax.set_title('behavior prediction - linear regression')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the nonlinear prediction captured finer temporal features in the data."
      ],
      "metadata": {
        "id": "H6tDJYUmkkEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: Can I use Rastermap in a scientific paper which requires rigorous quantification? How could I use it to quantify responses? Is it useful?**"
      ],
      "metadata": {
        "id": "VpJIsde0je3x"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "908px",
        "left": "1679px",
        "right": "20px",
        "top": "112px",
        "width": "479px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": true
    },
    "vscode": {
      "interpreter": {
        "hash": "998540cc2fc2836a46e99cd3ca3c37c375205941b23fd1eb4b203c48f2be758f"
      }
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
